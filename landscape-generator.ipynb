{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9644b239-2ac0-420f-a96f-4be3558aeb02",
      "metadata": {
        "id": "9644b239-2ac0-420f-a96f-4be3558aeb02"
      },
      "source": [
        "# **Landscape Generation**\n",
        "## **Project for the course of Deep Learning 2022-2023**\n",
        "\n",
        "**Author**: Jahrim Gabriele Cesario\n",
        "\n",
        "**GitHub Repository**: [link](https://github.com/jahrim/dl-project)\n",
        "\n",
        "**PowerPoint Presentation (Summary):** [link](https://github.com/jahrim/dl-project/blob/master/docs/landscape-generation.pptx)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this section, the content of this notebook will be briefly introduced."
      ],
      "metadata": {
        "id": "29Rm4f7exdDh"
      },
      "id": "29Rm4f7exdDh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Goal\n",
        "\n",
        "The goal of this project is to train a model capable of generating images representing different kinds of landscapes. In other words, the final result\n",
        "of this project should be a **Landscape Generator**."
      ],
      "metadata": {
        "id": "7JGYf33EFDJN"
      },
      "id": "7JGYf33EFDJN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "\n",
        "The training will be based on the [Landscape Recognition Dataset](https://www.kaggle.com/datasets/utkarshsaxenadn/landscape-recognition-image-dataset-12k-images) available on [Kaggle](https://www.kaggle.com/), which contains 12.000 images of **Coasts**, **Deserts**, **Forests**, **Glaciers** and **Mountains**, therefore making the model only capable of generating images of such landscape types."
      ],
      "metadata": {
        "id": "dhYByj45FGhE"
      },
      "id": "dhYByj45FGhE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strategy\n",
        "\n",
        "The architecture chosen for implementing the **Landscape Generator** will be\n",
        "that of a **Conditional Variational AutoEncoder (CVAE)**, made of two main\n",
        "components:\n",
        "- **Encoder**: mapping original images of landscapes into a compressed latent space;\n",
        "- **Decoder**: mapping points of the latent space into generated images of landscapes. This part of the model will play the role of **Landscape Generator**.\n",
        "\n",
        "To make it **Conditional**, the **Encoder** and **Decoder** will be trained with\n",
        "knowledge about the type of landscape they are processing, so that during prediction it will be possible to use the **Decoder** to generate a specific landscape type decided by the user.\n",
        "\n",
        "More formally, the **Decoder** will be a function $D$ with the following\n",
        "specification:\n",
        "\n",
        "$D(z,y) = x$\n",
        "\n",
        "where\n",
        "\n",
        "$z$ : is a point in the latent space $Z$\n",
        "\n",
        "$y$ : is a point representing one of the possible landscape types in $Y=\\{Coast, Desert, Forest, Glacier, Mountain\\}$\n",
        "\n",
        "$x$ : is a point representing the generated image.\n",
        "\n",
        "#### **Example**\n",
        "\n",
        "Consider the latent space $Z$ to be **bidimensional** and the representation of\n",
        "$Y$ to be the following:\n",
        "\n",
        "| Landscape | y         |\n",
        "|-----------|-----------|\n",
        "| Coast     | 0         |\n",
        "| Desert    | 1         |\n",
        "| Forest    | 2         |\n",
        "| Glacier   | 3         |\n",
        "| Mountain  | 4         |\n",
        "\n",
        "Then, $D$ should yield a behavior similar to the following:\n",
        "\n",
        "$D((0,0), 0) =$\n",
        "\n",
        "![Coast Image](https://drive.google.com/uc?export=view&id=1wLylelAmZ7DtqevFF0A-77nBYj-SbRs2)\n",
        "\n",
        "$D((0,0), 1) =$\n",
        "\n",
        "![Desert Image](https://drive.google.com/uc?export=view&id=151PmJtSqxIZ_VT_ghZc15SxFp216zc4l)"
      ],
      "metadata": {
        "id": "JXLu543zFQSR"
      },
      "id": "JXLu543zFQSR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "In order to evaluate the quality of the trained **Landscape Generator**,\n",
        "two main metrics will be considered:\n",
        "- **Loss**: which is a weighted sum of:\n",
        "\n",
        "    - **Reconstruction Error**: the error committed by the **CVAE** when\n",
        "      attempting to reconstruct the training, validation and test images.\n",
        "\n",
        "      A low **Reconstruction Error** tells that the model has learned to\n",
        "      correctly encode images into the latent space and to correctly decode\n",
        "      points from the latent space into images;\n",
        "\n",
        "    - **Regularization Error**: the error committed by the **CVAE** when\n",
        "      encoding the training, validation and test images into an irregular\n",
        "      latent space.\n",
        "\n",
        "      A low **Regularization Error** tells that the model has learned to\n",
        "      correctly encode images into a regular latent space, that is both\n",
        "      **complete** (i.e. most latent space points can be decoded into a\n",
        "      meaningful generated image) and **continuous** (i.e. similar points\n",
        "      in the latent space are decoded into similar generated images).\n",
        "\n",
        "- **Generation Quality**: an indicator of how simple it is to identify the\n",
        "  images generated by the **Landscape Generator** as the correct landscape\n",
        "  type.\n",
        "\n",
        "  The **Generation Quality** will be measured as the **accuracy** achieved\n",
        "  by a **Landscape Classifier** trained on the same dataset when applied to a\n",
        "  batch of images generated by the **Landscape Generator**.\n",
        "\n",
        "  An high **accuracy** of such classifier tells that the model has learned\n",
        "  a correct representation for all the possible landscape types.\n",
        "  \n",
        "  However, the **accuracy** won't measure the variance in the possible landscapes generated by the **Landscape Generator** (e.g. the generator may produce images of forests that are all classified as forests, but they all look the same).\n",
        "  \n",
        "  In other words, another quality metric should be introduced to consider also the variance in the generated images as a metric of **Generation Quality**."
      ],
      "metadata": {
        "id": "_Kx9cd7ldDqj"
      },
      "id": "_Kx9cd7ldDqj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Notebook Configuration\n",
        "\n",
        "In this section, the user can configure the major settings for executing this\n",
        "notebook."
      ],
      "metadata": {
        "id": "5P3Cabz6tviW"
      },
      "id": "5P3Cabz6tviW"
    },
    {
      "cell_type": "code",
      "source": [
        "# If 'False', use default values for skipping user interactions\n",
        "# with the notebook when possible\n",
        "is_interactive = False\n",
        "\n",
        "# If 'True', use verbose logger for tensorflow or other dependencies\n",
        "verbose = True\n",
        "\n",
        "# The random seed to make this notebook deterministic. Set to 'None' to make it non-deterministic.\n",
        "seed = 1234"
      ],
      "metadata": {
        "id": "lCxdWiQQuOM-"
      },
      "id": "lCxdWiQQuOM-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8f751faf-1c76-4822-91e2-c4147868bacb",
      "metadata": {
        "id": "8f751faf-1c76-4822-91e2-c4147868bacb"
      },
      "source": [
        "---\n",
        "\n",
        "## Environment Configuration\n",
        "\n",
        "In this section, the environment of the host machine will be configured, installing and importing the dependencies required to execute this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e6d514-9331-42ae-8ce6-e68f994804d6",
      "metadata": {
        "id": "19e6d514-9331-42ae-8ce6-e68f994804d6"
      },
      "source": [
        "### Install Dependencies\n",
        "\n",
        "Let's install the dependencies required to executed this notebook on the host\n",
        "machine. To do so, it is possible to use a combination of the **apt-get** and\n",
        "**pip** package managers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53ef17b1-38f3-4add-87b2-f56d29445491",
      "metadata": {
        "id": "53ef17b1-38f3-4add-87b2-f56d29445491"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get -y install graphviz\n",
        "!pip install h5py matplotlib opendatasets pandas tensorflow-datasets pydot pyyaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e394ecb2-1100-4cd2-ae21-e50c0f66b4b0",
      "metadata": {
        "id": "e394ecb2-1100-4cd2-ae21-e50c0f66b4b0"
      },
      "source": [
        "### Show Dependencies\n",
        "\n",
        "In order to list the dependencies installed in the host machine, it is possible\n",
        "to run the `pip freeze` command. This may prove useful to verify version compatibilities between several dependencies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "id": "dbYO7Hxcevgb"
      },
      "id": "dbYO7Hxcevgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "67ea39eb-e263-497e-8ba7-54f2be34fd2e",
      "metadata": {
        "id": "67ea39eb-e263-497e-8ba7-54f2be34fd2e"
      },
      "source": [
        "### Import Dependencies\n",
        "\n",
        "Once the dependencies are installed in the host machine, it is possible to\n",
        "leverage them in this notebook by importing them.\n",
        "\n",
        "In particular, **Keras (Tensorflow)** will be used as the machine learning framework\n",
        "for training the **Landscape Generator**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5deec880-1662-4245-9e9d-3bab3a272ba8",
      "metadata": {
        "id": "5deec880-1662-4245-9e9d-3bab3a272ba8"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import opendatasets as od\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as tfhub\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as layers\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Dependencies\n",
        "\n",
        "Finally, the imported dependencies can be configured for this specific notebook."
      ],
      "metadata": {
        "id": "gm-BgBQ_NWqd"
      },
      "id": "gm-BgBQ_NWqd"
    },
    {
      "cell_type": "code",
      "source": [
        "# If verbose, use tensorflow in DEBUG mode\n",
        "if verbose:\n",
        "    os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '2'\n",
        "    tf.keras.utils.enable_interactive_logging()"
      ],
      "metadata": {
        "id": "NaE0w0lHNsdd"
      },
      "id": "NaE0w0lHNsdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If a random seed is given, use it globally\n",
        "if seed:\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "WKBq_tBZNugN"
      },
      "id": "WKBq_tBZNugN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2fef7391-c02a-441d-bd9b-f0c4c779245c",
      "metadata": {
        "id": "2fef7391-c02a-441d-bd9b-f0c4c779245c"
      },
      "source": [
        "### GPU Support\n",
        "\n",
        "Another important step during the preparation of the environment is to verify\n",
        "that the host machine is capable of executing code using its GPU, which is\n",
        "crucial during training.\n",
        "\n",
        "First, let's display some information about the GPU of the host machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e577b9-bf37-4ec1-9d9b-223ab9752373",
      "metadata": {
        "id": "a0e577b9-bf37-4ec1-9d9b-223ab9752373"
      },
      "outputs": [],
      "source": [
        "print(\"Host GPU Support:\")\n",
        "!nvidia-smi\n",
        "\n",
        "# For more details use:\n",
        "# !nvidia-smi -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's verify that **Tensorflow** is properly configured to run on such\n",
        "GPU.\n",
        "\n",
        "If it is, the GPU of the host machine should be shown amongst the available\n",
        "GPUs detected by **Tensorflow**."
      ],
      "metadata": {
        "id": "wm3nAAvVqa9n"
      },
      "id": "wm3nAAvVqa9n"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tensorflow GPU Support:\")\n",
        "print(\"- Physical GPUs Available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"- Logical GPUs Available:\", tf.config.list_logical_devices('GPU'))\n",
        "print(\"- GPU Used:\", tf.test.gpu_device_name())\n",
        "print(\"- GPU Support:\", tf.test.is_built_with_gpu_support())\n",
        "print(\"- Cuda:\", tf.test.is_built_with_cuda())\n",
        "print(\"- ROCm:\", tf.test.is_built_with_rocm())\n",
        "print(\"- XLA\", tf.test.is_built_with_xla())"
      ],
      "metadata": {
        "id": "Nxywkx00WPqJ"
      },
      "id": "Nxywkx00WPqJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "KZUdB81bG_aK",
      "metadata": {
        "id": "KZUdB81bG_aK"
      },
      "source": [
        "### Prepare Directories\n",
        "\n",
        "Here, all the directories that will be used during the execution\n",
        "of the notebook are created.\n",
        "\n",
        "In particular, the following directories will be created:\n",
        "- **Dataset**: the directory where the training, validation and test sets\n",
        "  will be stored;\n",
        "- **Cache**: the directory where **Tensorflow** will store intermediate\n",
        "  processing of the dataset to achieve faster perfomance during training;\n",
        "- **Images**: the directory where **Tensorflow** will store the images\n",
        "  of the computational graphs of the trained models;\n",
        "- **Model**: the directory where **Tensorflow** will save the models\n",
        "  trained during the execution of this notebook.\n",
        "\n",
        "The paths to these directories can be changed in the code below, by\n",
        "modifying the `directories` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jq1pIM92HD__",
      "metadata": {
        "id": "Jq1pIM92HD__"
      },
      "outputs": [],
      "source": [
        "directories = {\n",
        "    \"dataset\": \"./.dataset/\",\n",
        "    \"cache\": \"./.cache/\",\n",
        "    \"images\": \"./.images/\",\n",
        "    \"model\": \"./.model/\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is already present, the notebook will avoid re-downloading it\n",
        "from Kaggle."
      ],
      "metadata": {
        "id": "KKUQlfufdevb"
      },
      "id": "KKUQlfufdevb"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory_already_exist = os.path.isdir(directories['dataset'])"
      ],
      "metadata": {
        "id": "zGNWFGmvcig2"
      },
      "id": "zGNWFGmvcig2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset has changed since the previous run of this notebook,\n",
        "the user should delete its cache, to recompute its records."
      ],
      "metadata": {
        "id": "mZ-df48RdQFp"
      },
      "id": "mZ-df48RdQFp"
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isdir(directories['cache']):\n",
        "    delete_cache = input(\"Delete cache? (Y/n)\") if is_interactive else \"\"\n",
        "    if not(delete_cache and delete_cache[0].lower() == 'n'):\n",
        "        print(\"Deleting cache...\")\n",
        "        shutil.rmtree(directories['cache'])\n",
        "        print(\"Cache deleted.\")"
      ],
      "metadata": {
        "id": "dN6H1bhCdFvU"
      },
      "id": "dN6H1bhCdFvU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's create the directories of the host environment."
      ],
      "metadata": {
        "id": "bvAWLlL3duJH"
      },
      "id": "bvAWLlL3duJH"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating directories...\")\n",
        "for name in directories:\n",
        "    try:\n",
        "        os.mkdir(directories[name])\n",
        "        print(directories[name], \"directory created.\")\n",
        "    except FileExistsError:\n",
        "        print(directories[name], \"directory already exists.\")"
      ],
      "metadata": {
        "id": "evPEcI3ecAcJ"
      },
      "id": "evPEcI3ecAcJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5efe457a-596d-4247-af72-b8e0f22a8a9b",
      "metadata": {
        "id": "5efe457a-596d-4247-af72-b8e0f22a8a9b"
      },
      "source": [
        "### Download Dataset\n",
        "\n",
        "Finally, as the last step of the environment configuration, let's download the\n",
        "dataset on which the model will be trained.\n",
        "\n",
        "First, we can define a utility function for moving and renaming files from\n",
        "the download directory to the dataset directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6edad1ee-efb4-4276-9c0f-71179c36a258",
      "metadata": {
        "id": "6edad1ee-efb4-4276-9c0f-71179c36a258"
      },
      "outputs": [],
      "source": [
        "def index_files(path, moveTo = None, i = 0):\n",
        "    \"\"\"Renames all files in the specified directory to an increasing index.\n",
        "\n",
        "    Description:\n",
        "    Renames all files inside the 'path' directory to an increasing index,\n",
        "    starting from 'i', eventually moving them to the 'moveTo' directory.\n",
        "    \"\"\"\n",
        "    if moveTo is None: moveTo = path\n",
        "    for filename in os.listdir(path):\n",
        "        name, extension = os.path.splitext(filename)\n",
        "        os.rename(path + \"/\" + filename, moveTo + \"/\" + str(i) + extension)\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, it is possible to leverage the [opendatasets](https://pypi.org/project/opendatasets/) library to download the [Landscape Recognition Dataset](https://www.kaggle.com/datasets/utkarshsaxenadn/landscape-recognition-image-dataset-12k-images) from [Kaggle](https://www.kaggle.com/). In order to do so, a **Kaggle Account** is required. In fact, **opendatasets** asks the user to input its **Kaggle Username** and **API Token** when downloading Kaggle datasets (refer to [Kaggle Authentication](https://www.kaggle.com/docs/api#getting-started-installation-&-authentication) for more information).\n",
        "\n",
        "When successfully authenticated, **opendatasets** will create a download\n",
        "directory and it will start downloading the dataset. The download directory will contain all the files of the dataset hosted on\n",
        "Kaggle, which includes different formats for representing the data of the dataset, such as [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord)s or JPEGs, together with the best classifier trained on the dataset,\n",
        "namely [BiT-LR-91-83.h5](https://www.kaggle.com/datasets/utkarshsaxenadn/landscape-recognition-image-dataset-12k-images).\n",
        "\n",
        "In this notebook, we'll use **TFRecords**, so all **TFRecords** are moved from the download directory to the dataset directory, while **BiT-LR-91-83.h5** is\n",
        "moved in the current directory and renamed to **landscape-classifier.h5**. Finally, the download directory is deleted.\n",
        "\n",
        "If the dataset directory was already present when running the notebook, it is assumed that the dataset has already been downloaded and the download is skipped."
      ],
      "metadata": {
        "id": "2PmdG9ctxGVM"
      },
      "id": "2PmdG9ctxGVM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a80064b-8304-4185-bbe4-4f132447a828",
      "metadata": {
        "id": "7a80064b-8304-4185-bbe4-4f132447a828"
      },
      "outputs": [],
      "source": [
        "download_directory = \"./landscape-recognition-image-dataset-12k-images\"\n",
        "download_dataset_directory = download_directory + \"/Landscape Classification/Landscape Classification/TFrecords/\"\n",
        "\n",
        "if not dataset_directory_already_exist:\n",
        "  print(\"Downloading dataset...\")\n",
        "  od.download(\"https://www.kaggle.com/datasets/utkarshsaxenadn/landscape-recognition-image-dataset-12k-images\")\n",
        "  index_files(path = download_dataset_directory + \"Train\", moveTo = directories['dataset'], i = 0)\n",
        "  index_files(path = download_dataset_directory + \"Valid\", moveTo = directories['dataset'], i = 10000)\n",
        "  index_files(path = download_dataset_directory + \"Test\", moveTo = directories['dataset'], i = 11500)\n",
        "  os.rename(download_directory + \"/BiT-LR-91-83.h5\", \"./landscape-classifier.h5\")\n",
        "  shutil.rmtree(download_directory)\n",
        "else:\n",
        "  print(\"Skipping dataset download: dataset directory already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c982c8-762d-4a1a-b391-bd3384ab41a9",
      "metadata": {
        "id": "74c982c8-762d-4a1a-b391-bd3384ab41a9"
      },
      "source": [
        "---\n",
        "\n",
        "## Utilities\n",
        "\n",
        "In this section, a set of utilities that will be used within the notebook is defined."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boolean Input\n",
        "\n",
        "Here, a function to ask the user for a boolean input is provided."
      ],
      "metadata": {
        "id": "rdToA0hgRwQu"
      },
      "id": "rdToA0hgRwQu"
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_input(question):\n",
        "    \"\"\"Ask the specified yes/no 'question' to the user.\"\"\"\n",
        "    if not is_interactive: print(question)\n",
        "    answer = input(question + ' (y/N)\\n') if is_interactive else \"\"\n",
        "    boolean_answer = answer and answer[0].lower() == 'y'\n",
        "    if boolean_answer:\n",
        "        print(\"Answer: [Yes]\")\n",
        "    else:\n",
        "        print(\"Answer: [No]\")\n",
        "    return boolean_answer"
      ],
      "metadata": {
        "id": "Vd199nmeRvuk"
      },
      "id": "Vd199nmeRvuk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count Records\n",
        "\n",
        "Here, a function that counts the records of a **tf.data.Dataset**, in case its size it's unknown."
      ],
      "metadata": {
        "id": "D0WmagG7hRtJ"
      },
      "id": "D0WmagG7hRtJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def count_records(dataset):\n",
        "    \"\"\"Count the number of records contained by the specified 'dataset'.\"\"\"\n",
        "    dataset_size = 0\n",
        "    for record in dataset: dataset_size = dataset_size + 1\n",
        "    return dataset_size"
      ],
      "metadata": {
        "id": "8A60Njn9hRU8"
      },
      "id": "8A60Njn9hRU8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log\n",
        "\n",
        "Here, a logging function to customize the logs within this notebook is provided."
      ],
      "metadata": {
        "id": "I90wRdpB0wP1"
      },
      "id": "I90wRdpB0wP1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc5ff86-63b5-4839-af71-45309334b345",
      "metadata": {
        "id": "bfc5ff86-63b5-4839-af71-45309334b345"
      },
      "outputs": [],
      "source": [
        "def log(*args):\n",
        "    \"\"\"Print the specified sequence of objects.\"\"\"\n",
        "    prefix = [\"[\" + str(datetime.utcnow()) + \"]:\"]\n",
        "    print(*(prefix + [*args]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot History\n",
        "\n",
        "Here, a function to plot the history of a training session is provided."
      ],
      "metadata": {
        "id": "huUnw8c_09W7"
      },
      "id": "huUnw8c_09W7"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history, metrics=[], skip=0):\n",
        "    \"\"\"\n",
        "    Plot the loss history of a training session.\n",
        "    'metrics' can be used to specify additional metrics to plot.\n",
        "    'skip' can be used to specify how many epochs will be skipped\n",
        "    from the start of the training in the plot.\n",
        "    \"\"\"\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    loss = history.history['loss'][skip:]\n",
        "    epoch_count=len(loss)\n",
        "    epochs=range(1+skip,1+skip+epoch_count)\n",
        "\n",
        "    line1,=ax1.plot(epochs,loss,label='loss',color='orange')\n",
        "    ax1.set_xlim([1, epochs[-1]])\n",
        "    ax1.set_ylim([0, max(loss)])\n",
        "    ax1.set_ylabel('loss',color = line1.get_color())\n",
        "    ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
        "    ax1.grid(True)\n",
        "    ax1.set_xlabel('Epochs')\n",
        "\n",
        "    for metric_name in metrics:\n",
        "        metric=history.history[metric_name][skip:]\n",
        "        line2,=ax1.plot(epochs,metric,label=metric_name)\n",
        "        ax1.set_ylim([0, max(ax1.get_ylim()[1], max(metric))])\n",
        "\n",
        "    _=ax1.legend(loc='best')"
      ],
      "metadata": {
        "id": "b2TlHpkz_B8Z"
      },
      "id": "b2TlHpkz_B8Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1ee9b3a9-955f-49c6-8dfa-12cb9d9babd0",
      "metadata": {
        "id": "1ee9b3a9-955f-49c6-8dfa-12cb9d9babd0"
      },
      "source": [
        "---\n",
        "\n",
        "## Preprocessing\n",
        "\n",
        "In this section, the dataset will be prepared for training using different\n",
        "preprocessing techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration\n",
        "\n",
        "In the code below, the user can configure how the **preprocessing** should be\n",
        "performed."
      ],
      "metadata": {
        "id": "eVSYbvrC_i0K"
      },
      "id": "eVSYbvrC_i0K"
    },
    {
      "cell_type": "code",
      "source": [
        "cropped_image_size = [256, 256, 3]   # The size of the dataset images after cropping\n",
        "resized_image_size = [128, 128, 3]   # The size of the dataset images after resizing\n",
        "image_size = resized_image_size      # The size of the dataset images after preprocessing"
      ],
      "metadata": {
        "id": "zQL6zR-jABs_"
      },
      "id": "zQL6zR-jABs_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "029e3f0f-fb92-4afc-a165-e77d3ec88a70",
      "metadata": {
        "id": "029e3f0f-fb92-4afc-a165-e77d3ec88a70"
      },
      "source": [
        "### Load Dataset\n",
        "\n",
        "First, an in-memory view of the dataset should be created. In order to do so, we'll rely on\n",
        "[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)s.\n",
        "\n",
        "In particular, the [tf.data.TFRecordDataset](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset) can be used to reference **TFRecord**s and to prepare a processing pipeline for the dataset, without loading\n",
        "its actual content in memory until training.\n",
        "\n",
        "Here, a **TFRecordDataset** is created from the list of **TFRecord** files that\n",
        "were previously moved in the dataset directory.\n",
        "\n",
        "First, let's collect the dataset files in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89314a4-a33d-4761-9250-ca752aae1cd6",
      "metadata": {
        "id": "d89314a4-a33d-4761-9250-ca752aae1cd6"
      },
      "outputs": [],
      "source": [
        "dataset_files = list(map(lambda f: os.path.join(directories[\"dataset\"], f), os.listdir(directories[\"dataset\"])))\n",
        "dataset_size = len(dataset_files)\n",
        "log(\"Dataset Size:\", dataset_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's shuffle the dataset files once to ensure that the following code\n",
        "won't rely on the order with which the data is presented.\n",
        "\n",
        "> _**Note**: since shuffling a **tf.data.Dataset** requires loading a buffer of the data of the dataset, a proper full shuffle of the dataset can only occur when the whole dataset can be loaded in memory. At this point of the notebook, the dataset consists only in a list of filenames, so a full shuffle can be easily performed._"
      ],
      "metadata": {
        "id": "TsQXsPF6Aq5V"
      },
      "id": "TsQXsPF6Aq5V"
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(dataset_files)"
      ],
      "metadata": {
        "id": "fV4l36L3AnSi"
      },
      "id": "fV4l36L3AnSi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's create a **TFRecordDataset** from the dataset files."
      ],
      "metadata": {
        "id": "SlKwcs0wBsGT"
      },
      "id": "SlKwcs0wBsGT"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.TFRecordDataset(dataset_files)"
      ],
      "metadata": {
        "id": "64e8gIccAfHe"
      },
      "id": "64e8gIccAfHe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse Dataset\n",
        "\n",
        "At this point, each record of the dataset is represented as the raw content of the corresponding **TFRecord**, which is akin to a binary format.\n",
        "\n",
        "Before the dataset can be used, its records should be parsed into a format that is suitable for training.\n",
        "\n",
        "The **TFRecord**s of the [Landscape Recognition Dataset](https://www.kaggle.com/datasets/utkarshsaxenadn/landscape-recognition-image-dataset-12k-images)\n",
        "contains two **features** per record:\n",
        "- **image**: the string encoding of a 256x256x3 JPEG image;\n",
        "- **label**: an integer indicating the type of landscape represented in the\n",
        "  image (0: Coast, 1: Desert, 2: Forest, 3: Glacier, 4: Mountain)."
      ],
      "metadata": {
        "id": "BFhIbBiIg5r8"
      },
      "id": "BFhIbBiIg5r8"
    },
    {
      "cell_type": "code",
      "source": [
        "# The features of a TFRecord\n",
        "features = {\n",
        "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'image': tf.io.FixedLenFeature([], tf.string),\n",
        "}\n",
        "\n",
        "# A map from label_ids to label_names (and viceversa)\n",
        "label_names = { 0: 'Coast', 1: 'Desert', 2: 'Forest', 3: 'Glacier', 4: 'Mountain' }\n",
        "label_ids = { name: id for id, name in label_names.items() }\n",
        "label_count = len(label_names)\n",
        "\n",
        "def label_id_of(label):\n",
        "    \"\"\"Return the id of the specified 'label', which is either its name or id.\"\"\"\n",
        "    return label_ids[label] if type(label) == str else label"
      ],
      "metadata": {
        "id": "AtUsXB-ghCpO"
      },
      "id": "AtUsXB-ghCpO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `tf.io.parse_single_example` it is possible to convert each **TFRecord**\n",
        "into a dictionary of its parsed content.\n",
        "\n",
        "Here, `preprocess_record` does exactly that, producing for each **TFRecord** a\n",
        "pair of **(Object, Label)** (i.e. **(x, y)**), which is standard when dealing with **tf.data.Datasets** in **Keras**."
      ],
      "metadata": {
        "id": "vDKoMoS4hltb"
      },
      "id": "vDKoMoS4hltb"
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_record(tfrecord):\n",
        "    \"\"\"Decode the specified raw TFRecord into a pair of (object, label)\"\"\"\n",
        "    xyi = tf.io.parse_single_example(tfrecord, features)\n",
        "    return (xyi['image'], xyi['label'])"
      ],
      "metadata": {
        "id": "eTGXFbnIhjki"
      },
      "id": "eTGXFbnIhjki",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(preprocess_record, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "CY9wgrw1-Hjm"
      },
      "id": "CY9wgrw1-Hjm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter Dataset\n",
        "\n",
        "For additional flexibility, it is also possible to exclude certain landscape types from the dataset, in order to analyze the performances of the model when the variance of the records in the dataset is reduced."
      ],
      "metadata": {
        "id": "E2otPa17vc1b"
      },
      "id": "E2otPa17vc1b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's define some filters that allows to extract only images representing specific landscape types from the dataset."
      ],
      "metadata": {
        "id": "EtrWxHvZv9Ne"
      },
      "id": "EtrWxHvZv9Ne"
    },
    {
      "cell_type": "code",
      "source": [
        "def has_label(label):\n",
        "    \"\"\"Filter out all the records without the specified 'label'.\"\"\"\n",
        "    label_id = label_id_of(label)\n",
        "    def apply(xi, yi): return tf.argmax(yi) == label_id if tf.rank(yi) == 1 else yi == label_id\n",
        "    return apply\n",
        "\n",
        "def has_not_label(label):\n",
        "    \"\"\"Filter out all the records with the specified 'label'.\"\"\"\n",
        "    has_label_apply = has_label(label)\n",
        "    def apply(xi, yi): return not has_label_apply(xi, yi)\n",
        "    return apply"
      ],
      "metadata": {
        "id": "fMRVWEa0v85-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fMRVWEa0v85-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the user can decide which landscape types will be excluded from the\n",
        "original dataset."
      ],
      "metadata": {
        "id": "foee0OoYwlxc"
      },
      "id": "foee0OoYwlxc"
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_labels = []\n",
        "for label_name in label_names.copy().values():\n",
        "    if boolean_input(\"Exclude \" + label_name + \"s from the dataset?\"):\n",
        "        dataset = dataset.filter(has_not_label(label_name))\n",
        "        excluded_labels.append(label_name)"
      ],
      "metadata": {
        "id": "e6w9l1Z8fJ_4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "e6w9l1Z8fJ_4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If any landscape type has been excluded, then we need to recompute the\n",
        "size of the dataset accordingly. Since, the landscape types in the dataset are equally balanced, this is as easy as multiplying its original size by the percentage of included landscape types."
      ],
      "metadata": {
        "id": "vcAUyMbvlDlT"
      },
      "id": "vcAUyMbvlDlT"
    },
    {
      "cell_type": "code",
      "source": [
        "if excluded_labels:\n",
        "    included_label_count = label_count - len(excluded_labels)\n",
        "    dataset_size = int(dataset_size * included_label_count / label_count)"
      ],
      "metadata": {
        "id": "xOUga9YKjCTR"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xOUga9YKjCTR"
    },
    {
      "cell_type": "code",
      "source": [
        "log(\"Filtered Dataset Size:\", dataset_size)"
      ],
      "metadata": {
        "id": "iOt0JF-elf9p"
      },
      "execution_count": null,
      "outputs": [],
      "id": "iOt0JF-elf9p"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, the information known about the labels of the dataset should be updated.\n",
        "In particular, the excluded label names must be removed from the known labels,\n",
        "while the label identifiers must be updated so that they progressively increase\n",
        "starting from 0.\n",
        "\n",
        "> _**Note**: such constraints on the label identifiers are only needed for\n",
        "**one-hot encoding**._"
      ],
      "metadata": {
        "id": "bskQJHO_2gES"
      },
      "id": "bskQJHO_2gES"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save original label names and identifiers\n",
        "original_label_names = label_names.copy()\n",
        "original_label_ids = label_ids.copy()\n",
        "\n",
        "if excluded_labels:\n",
        "    # Update label names and identifiers\n",
        "    i = 0\n",
        "    label_names.clear()\n",
        "    label_ids.clear()\n",
        "    for label_name in original_label_ids.keys():\n",
        "        if label_name not in excluded_labels:\n",
        "            label_names[i] = label_name\n",
        "            label_ids[label_name] = i\n",
        "            i = i + 1\n",
        "    label_count = included_label_count"
      ],
      "metadata": {
        "id": "jyDr4sEU2kvn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jyDr4sEU2kvn"
    },
    {
      "cell_type": "code",
      "source": [
        "log(\"Original Dataset Labels: \", original_label_ids)\n",
        "log(\"Filtered Dataset Labels: \", label_ids)"
      ],
      "metadata": {
        "id": "1KoGuHTY3T7W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1KoGuHTY3T7W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the trasformation on the label identifiers should also be reflected on the dataset. In order to do so, we can apply a discrete mapping function from\n",
        "the old identifiers contained in the dataset to the new ones.\n",
        "\n",
        "_**Note**: a discrete function can be modelled as a dictionary whose keys are\n",
        "elements of the **Domain** and whose values are elements of the **Codomain**._"
      ],
      "metadata": {
        "id": "VklRW8gYFqvI"
      },
      "id": "VklRW8gYFqvI"
    },
    {
      "cell_type": "code",
      "source": [
        "def discrete_map_labels(partial_discrete_map_fn, default=-1):\n",
        "    \"\"\"\n",
        "    Update the specified label using the specified 'discrete_map_fn', which is a dictionary from old\n",
        "    labels ids to new label ids. If no matching old label id is found for an id, then the id is\n",
        "    mapped to 'default'.\n",
        "    \"\"\"\n",
        "    discrete_map_fn_as_list = []\n",
        "    for old_id in range(0, max(list(partial_discrete_map_fn.keys()))+1):\n",
        "        discrete_map_fn_as_list.append(partial_discrete_map_fn.get(old_id, default))\n",
        "    tf_discrete_map_fn = tf.constant(discrete_map_fn_as_list, dtype=tf.int64)\n",
        "    def apply(xi, yi): return (xi, tf.gather(tf_discrete_map_fn, yi))\n",
        "    return apply"
      ],
      "metadata": {
        "id": "Q9wZwEUcF5rK"
      },
      "id": "Q9wZwEUcF5rK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define the discrete mapping function from old to new label identifiers."
      ],
      "metadata": {
        "id": "DoNZ29mavMw0"
      },
      "id": "DoNZ29mavMw0"
    },
    {
      "cell_type": "code",
      "source": [
        "old_id_to_new_id = {original_label_ids[label_name]: label_ids[label_name] for label_name in label_ids}"
      ],
      "metadata": {
        "id": "WwWBaLEhvHnM"
      },
      "id": "WwWBaLEhvHnM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(old_id_to_new_id)"
      ],
      "metadata": {
        "id": "OWjbzXarvXMv"
      },
      "id": "OWjbzXarvXMv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, such mapping function is applied to the labels of the dataset."
      ],
      "metadata": {
        "id": "NNgaaoPsvbsQ"
      },
      "id": "NNgaaoPsvbsQ"
    },
    {
      "cell_type": "code",
      "source": [
        "if excluded_labels:\n",
        "    dataset = dataset.map(discrete_map_labels(old_id_to_new_id), num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "uermImq7Uy43"
      },
      "id": "uermImq7Uy43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "00c10e1c-ca14-4d72-b0ea-24a25dd484cd",
      "metadata": {
        "id": "00c10e1c-ca14-4d72-b0ea-24a25dd484cd"
      },
      "source": [
        "### Preprocess Dataset\n",
        "\n",
        "After filtering the dataset, it is finally possible to apply several preprocessing techniques to its data.\n",
        "\n",
        "First, let's decode the string encoding of the **image** feature into a JPEG,\n",
        "while cropping the image to ensure that all images will have the same size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d234b9d-9340-4dc2-b693-3fb78f794b0e",
      "metadata": {
        "id": "8d234b9d-9340-4dc2-b693-3fb78f794b0e"
      },
      "outputs": [],
      "source": [
        "def decode_and_crop_image(crop_window, channels):\n",
        "    \"\"\"Decode the specified string image to JPEG and crop it with the specified 'crop_window'.\"\"\"\n",
        "    def apply(xi, yi):\n",
        "        return (tf.image.decode_and_crop_jpeg(xi, crop_window=crop_window, channels=channels), yi)\n",
        "    return apply"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's resize the image to a size that is suitable for training."
      ],
      "metadata": {
        "id": "hpATK28rJYHK"
      },
      "id": "hpATK28rJYHK"
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_image(new_size):\n",
        "    \"\"\"Resize the specified image to the specified 'size'.\"\"\"\n",
        "    def apply(xi, yi):\n",
        "        return (tf.image.resize(xi, size=new_size, method=tf.image.ResizeMethod.BILINEAR), yi)\n",
        "    return apply"
      ],
      "metadata": {
        "id": "PC_i1GDXgq1t"
      },
      "id": "PC_i1GDXgq1t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that, let's normalize the RGB values of the images from the range\n",
        "$[0..255]$ to the range $[0..1]$, as a good practice for increased\n",
        "training stability."
      ],
      "metadata": {
        "id": "c3pTnC8OJjjU"
      },
      "id": "c3pTnC8OJjjU"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_image(xi, yi):\n",
        "    \"\"\"Normalize the specified image from the space [0..255]^N to the space [0..1]^N\"\"\"\n",
        "    return (tf.cast(xi, tf.float32) / 255, yi)"
      ],
      "metadata": {
        "id": "0CJFjjeIgsVa"
      },
      "id": "0CJFjjeIgsVa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's represent the label of the images as **one-hot vectors** to forbid the model from inferring inexistent correlations between **scalar** labels.\n",
        "\n",
        "> **Example** \\\\\n",
        "If $0$ means **Coast**, $1$ means **Desert** and $5$ means **Mountain**, the model could infer that coasts are more similar to deserts than mountains just because $|0-1| < |0-5|$. Instead, one-hot encoding ensures that all labels are equidistant between each other.\n",
        "\n",
        "In addition, a function is provided to map a probability distribution on labels to the name of the most probable label and its probability."
      ],
      "metadata": {
        "id": "zttZzTV7Kwh7"
      },
      "id": "zttZzTV7Kwh7"
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_one_hot_encoding(xi, yi):\n",
        "    \"\"\"Encode the specified label using one-hot-encoding\"\"\"\n",
        "    return (xi, tf.one_hot(yi, label_count, dtype=tf.float32))\n",
        "\n",
        "def one_hot_encoding_to_label_name(yi, attach_probability = True):\n",
        "    \"\"\"Decode a probability distribution on labels to the human-friendly name of the most probable label.\"\"\"\n",
        "    label_index = tf.argmax(yi).numpy()\n",
        "    label_name = label_names[label_index]\n",
        "    return \"{} ({:.2f}%)\".format(label_name, yi[label_index].numpy()*100) if attach_probability else label_name"
      ],
      "metadata": {
        "id": "eQpryD5sgu5K"
      },
      "id": "eQpryD5sgu5K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, all the aforementioned preprocessing techniques are applied on the dataset."
      ],
      "metadata": {
        "id": "_gx16xCmM_61"
      },
      "id": "_gx16xCmM_61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8236bf83-2106-4f3d-ab6b-c1d616ba2ecd",
      "metadata": {
        "id": "8236bf83-2106-4f3d-ab6b-c1d616ba2ecd"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(decode_and_crop_image(crop_window=[0,0,cropped_image_size[0],cropped_image_size[1]], channels=cropped_image_size[2]), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.map(resize_image(new_size=resized_image_size[0:2]), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.map(normalize_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.map(label_to_one_hot_encoding, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227f743f-62f5-404f-9955-7a4272c726e8",
      "metadata": {
        "id": "227f743f-62f5-404f-9955-7a4272c726e8"
      },
      "source": [
        "### Visualize Dataset\n",
        "\n",
        "After preprocessing the dataset, we can finally take a look at the data that\n",
        "will be fed to the model during training.\n",
        "\n",
        "First, let's analyze the shape of the records of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aac12f19-1677-47bf-9f01-aad64812d6f7",
      "metadata": {
        "id": "aac12f19-1677-47bf-9f01-aad64812d6f7"
      },
      "outputs": [],
      "source": [
        "for (x0, y0) in dataset.take(1):\n",
        "    log(\"Object Shape:\", x0.shape)\n",
        "    log(\"Label Shape:\", y0.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's look at the images that will be observed by the model during training."
      ],
      "metadata": {
        "id": "XCTV0j--OYsc"
      },
      "id": "XCTV0j--OYsc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d923ead-f9c1-47c3-bbfc-c512ca46a483",
      "metadata": {
        "id": "2d923ead-f9c1-47c3-bbfc-c512ca46a483"
      },
      "outputs": [],
      "source": [
        "def show_dataset_sample(dataset, n_rows=1, n_cols=1, image_header=True, show_label_probability=True, figsize=None):\n",
        "    \"\"\"Display a grid of a number of samples from 'dataset' equals to 'n_rows*n_cols'.\"\"\"\n",
        "    if figsize == None: figsize = (15, 3*n_rows)\n",
        "    xys = iter(dataset)\n",
        "    fig, axs = plt.subplots(n_rows, n_cols, squeeze=False, figsize = figsize)\n",
        "    for row in range(0, n_rows):\n",
        "        for col in range(0, n_cols):\n",
        "            (xi, yi) = next(xys)\n",
        "            axs[row, col].imshow(xi)\n",
        "            if image_header: axs[row, col].title.set_text(one_hot_encoding_to_label_name(yi, show_label_probability))\n",
        "            axs[row, col].axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(dataset, n_rows=5, n_cols=5, image_header=True, show_label_probability=True)"
      ],
      "metadata": {
        "id": "Ab0zSASrhRg2"
      },
      "id": "Ab0zSASrhRg2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that the images present an **high variance**, even between the same landscape types.\n",
        "\n",
        "Moreover, some images contain a lot of **noise**, usually the subject that is situated in the landscape portraited in the image (e.g. people, buildings, picture frames...); while others contain **mixed landscapes** (e.g. mixes of mountainous landscapes with forestal landscapes...).\n",
        "\n",
        "Finally, some images are completely misleading on purpose, since the dataset was\n",
        "prepared for a classification challenge (e.g. an indoor image of a water bottle branded 'Glacier', labeled as a **Glacier**...)."
      ],
      "metadata": {
        "id": "AMhBYVY_BNWw"
      },
      "id": "AMhBYVY_BNWw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Split Dataset\n",
        "\n",
        "In this section, the dataset will be split into three datasets for training, validation and testing of the model."
      ],
      "metadata": {
        "id": "QwTW4hw7QEJ_"
      },
      "id": "QwTW4hw7QEJ_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration\n",
        "\n",
        "In the code below, the user can change the relative sizes of the **training**, **validation** and **test sets**."
      ],
      "metadata": {
        "id": "hU8lYCLvr_ni"
      },
      "id": "hU8lYCLvr_ni"
    },
    {
      "cell_type": "code",
      "source": [
        "relative_training_set_size = 0.80\n",
        "relative_validation_set_size = 0.10\n",
        "relative_test_set_size = 0.10"
      ],
      "metadata": {
        "id": "v13cZBrOsBtv"
      },
      "id": "v13cZBrOsBtv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homogenize Dataset Distribution\n",
        "\n",
        "Before splitting the dataset into training, validation and\n",
        "test sets, it is necessary to make the distribution of data\n",
        "in the dataset homogeneous.\n",
        "\n",
        "Otherwise, when splitting the dataset, it may happen that\n",
        "the classes of the dataset are contained in disproportionate amounts in the training set (e.g. in extreme cases, the training set may contain no images related to deserts, because all the deserts are contained in the validation and test sets...)."
      ],
      "metadata": {
        "id": "4QtcUSahXknE"
      },
      "id": "4QtcUSahXknE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's extract all the images related to each landscape type into separate datasets."
      ],
      "metadata": {
        "id": "sUDSXe7B5Fis"
      },
      "id": "sUDSXe7B5Fis"
    },
    {
      "cell_type": "code",
      "source": [
        "landscape_datasets = []\n",
        "for label_name in label_ids:\n",
        "    landscape_datasets.append(dataset.filter(has_label(label_name)))"
      ],
      "metadata": {
        "id": "Lf_efO7RVmZS"
      },
      "id": "Lf_efO7RVmZS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's recreate the dataset so that its data is\n",
        "generated by alternating the data from the each of the previous landscape datasets (e.g. first it picks a record from the coast dataset, then from the desert dataset, then from the forest dataset and so on...).\n",
        "\n",
        "> _**Note**: this homogeneous distribution is only used for splitting the data into training, validation and test sets. During training, the data will be shuffled to prevent such distribution from influencing the training._"
      ],
      "metadata": {
        "id": "mR6h4XO45WBK"
      },
      "id": "mR6h4XO45WBK"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.choose_from_datasets(\n",
        "    landscape_datasets,\n",
        "    tf.data.Dataset.range(len(landscape_datasets)).repeat()\n",
        ")"
      ],
      "metadata": {
        "id": "ZOnjToc0XuVM"
      },
      "id": "ZOnjToc0XuVM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if we take a look at the dataset, we can observe that each landscape type repeats after a fixed amount of images."
      ],
      "metadata": {
        "id": "REznDD-J7Y5w"
      },
      "id": "REznDD-J7Y5w"
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(dataset, n_rows=5, n_cols=label_count, image_header=True, show_label_probability=True)"
      ],
      "metadata": {
        "id": "yvdpG9YcfWRE"
      },
      "id": "yvdpG9YcfWRE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training, Validation & Test Sets\n",
        "\n",
        "With the landscape types uniformly distributed within the dataset, it is possible to partition the dataset into training, validation and test sets simply by taking and skipping records of the dataset."
      ],
      "metadata": {
        "id": "7WVjGNgdpQCW"
      },
      "id": "7WVjGNgdpQCW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's compute the absolute sizes of the training, validation and test sets."
      ],
      "metadata": {
        "id": "_E5zpRypsqVS"
      },
      "id": "_E5zpRypsqVS"
    },
    {
      "cell_type": "code",
      "source": [
        "training_set_size = int(relative_training_set_size * dataset_size)\n",
        "validation_set_size = int(relative_validation_set_size * dataset_size)\n",
        "test_set_size = int(relative_test_set_size * dataset_size)"
      ],
      "metadata": {
        "id": "V9-ZltI4sC2I"
      },
      "id": "V9-ZltI4sC2I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log(\"Training Set Size:\", training_set_size)\n",
        "log(\"Validation Set Size:\", validation_set_size)\n",
        "log(\"Test Set Size:\", test_set_size)"
      ],
      "metadata": {
        "id": "AjxaXxrsswpW"
      },
      "id": "AjxaXxrsswpW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's partition the dataset."
      ],
      "metadata": {
        "id": "X0FOFiPSs4gA"
      },
      "id": "X0FOFiPSs4gA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a06d5b1e-89f2-4f35-aec7-f8c4a7796c93"
      },
      "outputs": [],
      "source": [
        "training_set = dataset.take(training_set_size)\n",
        "validation_set = dataset.skip(training_set_size).take(validation_set_size)\n",
        "test_set = dataset.skip(training_set_size + validation_set_size).take(test_set_size)"
      ],
      "id": "a06d5b1e-89f2-4f35-aec7-f8c4a7796c93"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Data Augmentation\n",
        "\n",
        "In this section, a set of augmentation techniques will be provided, so that they\n",
        "may optionally be applied to the training set before training, in an attempt to improve the model performance."
      ],
      "metadata": {
        "id": "m5C5tmKSuA1i"
      },
      "id": "m5C5tmKSuA1i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greyscale Augmentation\n",
        "\n",
        "The **Grayscale Augmentation** consists in extending the training set with the\n",
        "grayscale version of each one of its images.\n",
        "\n",
        "The purpose of such augmentation would be to reduce the amount of focus put by\n",
        "the model on the colors of the landscapes during training, so that it may learn\n",
        "also other features characterizing each landscape type.\n"
      ],
      "metadata": {
        "id": "vrdEG2iyMQR6"
      },
      "id": "vrdEG2iyMQR6"
    },
    {
      "cell_type": "code",
      "source": [
        "def to_grayscale(xi, yi):\n",
        "    \"\"\"Map an RGB image to grayscale without reducing the number of channels.\"\"\"\n",
        "    return (tf.repeat(tf.reduce_mean(xi, axis=2, keepdims=True), repeats=3, axis=2), yi)"
      ],
      "metadata": {
        "id": "b4lHC1v_Mo5N"
      },
      "id": "b4lHC1v_Mo5N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grayscale_training_set = training_set.map(to_grayscale, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "tbG_liR1Qds-"
      },
      "id": "tbG_liR1Qds-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(grayscale_training_set, n_rows=5, n_cols=5, show_label_probability=True)"
      ],
      "metadata": {
        "id": "nrHQ6CudQ2Nl"
      },
      "id": "nrHQ6CudQ2Nl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cut-Out Augmentation\n",
        "\n",
        "The **Cut-Out Augmentation** consists in extending the training set with new images\n",
        "obtained by removing random portions of the original images, adjusting the\n",
        "corresponding labels proportionally.\n",
        "\n",
        "The purpose of such augmentation would be to increase the robustness of the\n",
        "model, by reducing its perception of the images, therefore forcing it to focus\n",
        "on different areas of the same image.\n",
        "\n",
        "The provided implementation of **Cut-Out** allows to generate for each image\n",
        "of the training set a set of augmented images obtained by sliding a **mask** of a\n",
        "specified size by the specified horizontal and vertical **strides**."
      ],
      "metadata": {
        "id": "V2k4CJ2jgaGm"
      },
      "id": "V2k4CJ2jgaGm"
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_out(mask_shape, strides):\n",
        "    \"\"\"\n",
        "    Generate new images by applying cut-out to the specified image.\n",
        "\n",
        "    The images are generated by applying a mask with the specified\n",
        "    'mask_shape' in all possible positions using the specified 'strides'.\n",
        "    \"\"\"\n",
        "    cut_out_percentage = tf.cast(mask_shape[0] * mask_shape[1] / (image_size[0] * image_size[1]), dtype=tf.float32)\n",
        "    base_mask = tf.fill(value=0.0, dims=(mask_shape[0], mask_shape[1], image_size[2]))\n",
        "\n",
        "    mask_max_padding = (image_size[0] - mask_shape[0], image_size[1] - mask_shape[1])\n",
        "    n_strides = (int(mask_max_padding[0] / strides[0]), int(mask_max_padding[1] / strides[1]))\n",
        "    masks = []\n",
        "    for stride_x in range(0, n_strides[0]+1):\n",
        "        left = stride_x*strides[0]\n",
        "        right = mask_max_padding[0] - left\n",
        "        for stride_y in range(0, n_strides[0]+1):\n",
        "            top = stride_y*strides[1]\n",
        "            bottom = mask_max_padding[1] - top\n",
        "            masks.append(tf.pad(base_mask, paddings=[[left,right],[top,bottom],[0,0]], constant_values=1))\n",
        "    masks = tf.stack(masks)\n",
        "\n",
        "    def apply(xi, yi):\n",
        "        new_images = tf.data.Dataset.from_tensor_slices(tf.unstack(tf.repeat([xi], repeats=masks.shape[0], axis=0) * masks))\n",
        "        new_labels = tf.data.Dataset.from_tensor_slices(tf.repeat([yi*(1-cut_out_percentage)], repeats=masks.shape[0], axis=0))\n",
        "        return tf.data.Dataset.zip(new_images, new_labels)\n",
        "    return apply"
      ],
      "metadata": {
        "id": "u5gYQ8VRuAEn"
      },
      "id": "u5gYQ8VRuAEn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code below, it is possible to configure the **Cut-Out Augmentation** by\n",
        "specifying the size of the mask as `mask_shape` and the `strides` of its\n",
        "application."
      ],
      "metadata": {
        "id": "wqcAmbQTl-Kl"
      },
      "id": "wqcAmbQTl-Kl"
    },
    {
      "cell_type": "code",
      "source": [
        "mask_relative_size = 0.333\n",
        "mask_shape = (int(image_size[0]*mask_relative_size), int(image_size[1]*mask_relative_size))\n",
        "strides = mask_shape"
      ],
      "metadata": {
        "id": "hbImS82DXpxn"
      },
      "id": "hbImS82DXpxn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cut_out_training_set = training_set.flat_map(cut_out(mask_shape=mask_shape, strides=strides))"
      ],
      "metadata": {
        "id": "4XrV423blD1_"
      },
      "id": "4XrV423blD1_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(cut_out_training_set, n_rows=6, n_cols=3)"
      ],
      "metadata": {
        "id": "vbip4KLvYjrA"
      },
      "id": "vbip4KLvYjrA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augment Training Set\n",
        "\n",
        "Here, the training set may be augmented by applying the aforementioned augmentation\n",
        "techniques at the user's discretion."
      ],
      "metadata": {
        "id": "rAQXZcx1i2KB"
      },
      "id": "rAQXZcx1i2KB"
    },
    {
      "cell_type": "code",
      "source": [
        "augmentations = []\n",
        "\n",
        "use_grayscale_augmentation = boolean_input('Use grayscale augmentation?')\n",
        "if use_grayscale_augmentation: augmentations.append(grayscale_training_set)\n",
        "\n",
        "use_cut_out_augmentation = boolean_input('Use cut-out augmentation?')\n",
        "if use_cut_out_augmentation: augmentations.append(cut_out_training_set)"
      ],
      "metadata": {
        "id": "75yex5pcnXXP"
      },
      "id": "75yex5pcnXXP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if augmentations:\n",
        "    log(\"Using augmented training set.\")\n",
        "    training_set = tf.data.Dataset.sample_from_datasets([training_set] + augmentations)\n",
        "    training_set_size = count_records(training_set)\n",
        "    log(\"Augmented training set size:\", training_set_size)\n",
        "else:\n",
        "    log(\"Using original training set.\")"
      ],
      "metadata": {
        "id": "ZS1dLr0KTHfM"
      },
      "id": "ZS1dLr0KTHfM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at a sample of the training set in its final state."
      ],
      "metadata": {
        "id": "U2sTu7O7KAI7"
      },
      "id": "U2sTu7O7KAI7"
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(training_set, n_rows=5, n_cols=5)"
      ],
      "metadata": {
        "id": "RtDJLJz6J_q2"
      },
      "id": "RtDJLJz6J_q2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "eae1d806-51c4-45a1-8aea-848e439f6e4d",
      "metadata": {
        "id": "eae1d806-51c4-45a1-8aea-848e439f6e4d"
      },
      "source": [
        "---\n",
        "\n",
        "## Model Definition\n",
        "\n",
        "In this section, a description and definition of the model used for\n",
        "implementing the **Landscape Generator** will be provided."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "408d24d4-7979-4e1f-b7f7-4c00032e6936",
      "metadata": {
        "id": "408d24d4-7979-4e1f-b7f7-4c00032e6936"
      },
      "source": [
        "### Configuration\n",
        "\n",
        "Below, a list of the **hyperparameters** that the user can set to modify the\n",
        "**architecture** of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cde6d60-6e18-48f3-88a6-8166eb747e24",
      "metadata": {
        "id": "6cde6d60-6e18-48f3-88a6-8166eb747e24"
      },
      "outputs": [],
      "source": [
        "condition_size = label_count    # Size of the condition (possible classes of the input)\n",
        "code_size = 64                  # Size of the code in the latent space (i.e. #dimensions of the latent space)\n",
        "\n",
        "cnn_activation = 'relu'         # Activation function of the convolutional layers in the encoder and decoder\n",
        "latent_activation = None        # Activation function used by the encoder to generate the mean and variance of multi-normal distributions\n",
        "output_activation = 'sigmoid'   # Activation function used by the decoder to generate the image of a landscape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture\n",
        "\n",
        "The implemented model has the architecture illustrated below."
      ],
      "metadata": {
        "id": "mTHH-hJkeKNy"
      },
      "id": "mTHH-hJkeKNy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![CVAE](https://drive.google.com/uc?export=view&id=1En-N_ibDOIh5kfoZCgq64L9tJcGJoA8O)"
      ],
      "metadata": {
        "id": "UOn8KAQqeD5j"
      },
      "id": "UOn8KAQqeD5j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoding\n",
        "\n",
        "The model takes in input an RGB image of a landscape $x$ and the type of landscape $y$ represented by the image.\n",
        "\n",
        "First, the landscape type $y$ is reshaped using an ad-hoc transformation, so that it may be concatenated to the landscape image $x$.\n",
        "\n",
        "The concatenation $xy$ is then fed to the **Encoder Core** $EC$, whose purpose is to map the landscape image $x$ and its type $y$ into multinormal distributions in the latent space $Z$.\n",
        "\n",
        "A multinormal distribution can be uniquely identified by two main values: the mean of the distribution $\\mu$, which tells the position of the center of the distribution in the latent space, and the covariance matrix $\\Sigma$, which tells the shape of the distribution.\n",
        "\n",
        "Basically, as the **Encoder** $E$ observes more data, it will project different distributions into the latent space $Z$, covering $Z$ as the training goes on.\n",
        "\n",
        "> _**Note**: to reduce computation time during training, the **Encoder** will\n",
        "ignore covariances in $\\Sigma$. In other words, it will only learn the diagonal of $\\Sigma$, namely the **variance** vector $^2$._\n",
        "\n",
        "> _**Note**: to increase the stability during training, the **Encoder** will\n",
        "learn the **logaritmic variance** $ln(^2)$, instead of directly the **variance**  $^2$._"
      ],
      "metadata": {
        "id": "YewGqisJfRYH"
      },
      "id": "YewGqisJfRYH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoding\n",
        "\n",
        "Using the latent space $Z$ generated by the **Encoder**, it is possible to\n",
        "sample random points $z$ (or **codes**) to train a **Decoder** $D$ for reconstructing the original landscapes from their latent representations. During prediction, the same can be done for generating new landscapes.\n",
        "\n",
        "By feeding the concatenation $zy$ to the **Decoder**, instead of just the code $z$, the **Decoder** will learn to reconstruct the original landscapes with knowledge about their landscape types. This will let the user specify what\n",
        "type of landscape he wants to generate when using the **Decoder** during\n",
        "prediction.\n",
        "\n",
        "In particular, the concatenation $zy$ is processed by the **Decoder Core** $DC$, whose purpose is to map the code $z$ and the landscape type $y$ into the corresponding landscape image $x'$."
      ],
      "metadata": {
        "id": "lVgXTRvllgC8"
      },
      "id": "lVgXTRvllgC8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sampling\n",
        "\n",
        "During training, it is not possible to use standard sampling techniques for\n",
        "extracting points from the latent space $Z$, as most of them require generating\n",
        "random numbers, which is a non-differentiable operation, breaking training\n",
        "algorithms like **Gradient Descent**, which relies on the differentiability of the computation graph of the neural network.\n",
        "\n",
        "This problem can be solved using the **Reparametrization Trick**, which consists\n",
        "in treating all non-differentiable operations as inputs of the computation graph of the neural network. In fact, the error does not need to be backpropagated to the inputs of the graph.\n",
        "\n",
        "In the case of **Sampling**, this can be achieved with the following expansion\n",
        "of the computation graph:\n",
        "\n",
        "- From:\n",
        "\n",
        "  $z \\sim N(, )$ : Intermediate Node\n",
        "\n",
        "- To:\n",
        "\n",
        "  $ \\sim N(0^{k}, I_{k})$ : Input Node\n",
        "\n",
        "  $z =  \\cdot  +  $ : Intermediate Node\n",
        "\n",
        "  where $k$ is the number of dimensions of the latent space $Z$.\n",
        "\n",
        "> _**Note**: if we consider the **logaritmic variance** $ln(^2)$ instead of the **covariance matrix** $$, then $z$ can be sampled using the following formula: \\\\\n",
        "$z = e^{\\frac{1}{2} \\cdot ln(^2)} \\odot  + $_"
      ],
      "metadata": {
        "id": "pm9B8ecf6r4C"
      },
      "id": "pm9B8ecf6r4C"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layers\n",
        "\n",
        "First, let's define some custom layers that will be used for building the model."
      ],
      "metadata": {
        "id": "V2J3yip0m0YA"
      },
      "id": "V2J3yip0m0YA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multinormal Sampling\n",
        "\n",
        "The **Multinormal Sampling Layer** extracts random points from a batch of multinormal probability distributions using the **Reparametrization Trick**,\n",
        "just as described before."
      ],
      "metadata": {
        "id": "wyQc5rkLnE8e"
      },
      "id": "wyQc5rkLnE8e"
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class MultinormalSamplingLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A layer that takes in input the `mean` and `log_variance` of a batch of\n",
        "    multinormal distributions and produces as output a batch of random points\n",
        "    extracted from such distributions.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def sample(mean, log_variance):\n",
        "        batch_size = K.shape(mean)[0]\n",
        "        dim = K.int_shape(mean)[1]\n",
        "        epsilon = K.random_normal(shape=(batch_size, dim), mean=0., stddev=1.0)\n",
        "        z = K.exp(0.5 * log_variance) * epsilon + mean\n",
        "        return z\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return MultinormalSamplingLayer.sample(inputs[0], inputs[1])"
      ],
      "metadata": {
        "id": "9hZQy3jPtR42"
      },
      "id": "9hZQy3jPtR42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use it to extract some random points from a batch of **standard** multinormal distributions."
      ],
      "metadata": {
        "id": "v4UJO3ZTdGQ3"
      },
      "id": "v4UJO3ZTdGQ3"
    },
    {
      "cell_type": "code",
      "source": [
        "sample_count = 20000\n",
        "std_mean = tf.repeat([tf.constant([0.0, 0.0])], repeats=sample_count, axis=0)\n",
        "std_log_variance = tf.repeat([tf.constant([0.0, 0.0])], repeats=sample_count, axis=0)\n",
        "sample_points = MultinormalSamplingLayer()([std_mean, std_log_variance])"
      ],
      "metadata": {
        "id": "Mb2Tq2irucRZ"
      },
      "id": "Mb2Tq2irucRZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.figure(figsize=(5,5)), plt.axes()\n",
        "axes.set_aspect(\"equal\")\n",
        "_ = plt.plot(sample_points[:, 0], sample_points[:, 1], ',', color=\"orange\", zorder=0)\n",
        "_ = plt.plot([0], [0], '.', color=\"black\", zorder=0)\n",
        "_ = plt.axvline(color=\"black\", linewidth=1)\n",
        "_ = plt.axhline(color=\"black\", linewidth=1)"
      ],
      "metadata": {
        "id": "6rkOGQW0l0Ta"
      },
      "id": "6rkOGQW0l0Ta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the graph, we can see that most of the sample points are positioned inside a circular cluster centered in the origin $(0,0)$ as expected."
      ],
      "metadata": {
        "id": "xHqil4fjeWD9"
      },
      "id": "xHqil4fjeWD9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "\n",
        "Let's start building the **CVAE** from the **Encoder**.\n",
        "\n",
        "First, let's define the **Encoder Core**, whose architecture is a simple **CNN**.\n",
        "\n",
        "> **Note**: the architecture should be designed to compress the original images\n",
        "while retaining most of their information. A simple way to do that is to apply a small compression, extracting a large number of features from the images."
      ],
      "metadata": {
        "id": "UQUUrmB8PuFs"
      },
      "id": "UQUUrmB8PuFs"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_encoder_core(activation):\n",
        "    return keras.Sequential(\n",
        "        name=\"Encoder_Core\",\n",
        "        layers=[\n",
        "            layers.Conv2D(name=\"EC_Convolution1\", filters=16, kernel_size=(3, 3), strides=1, padding='same', activation=activation),\n",
        "            layers.Conv2D(name=\"EC_Convolution2\", filters=32, kernel_size=(3, 3), strides=1, padding='same', activation=activation),\n",
        "            layers.Conv2D(name=\"EC_Convolution3\", filters=64, kernel_size=(3, 3), strides=2, padding='same', activation=activation),\n",
        "            layers.Conv2D(name=\"EC_Convolution4\", filters=128, kernel_size=(3, 3), strides=1, padding='same', activation=activation),\n",
        "            layers.Conv2D(name=\"EC_Convolution5\", filters=256, kernel_size=(3, 3), strides=1, padding='same', activation=activation),\n",
        "            layers.Flatten(name=\"OriginalImage_Features\"),\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "s50zcHypPtMg"
      },
      "id": "s50zcHypPtMg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's define the **Encoder**, which contains the **Encoder Core**."
      ],
      "metadata": {
        "id": "wC2zW4tYfPLv"
      },
      "id": "wC2zW4tYfPLv"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cvae_encoder(image_shape, condition_size, code_size, activation, latent_activation):\n",
        "    original_image = layers.Input(name=\"OriginalImage\", shape=image_shape)\n",
        "    condition = layers.Input(name=\"Condition\", shape=condition_size)\n",
        "\n",
        "    condition_image = layers.Dense(name=\"ImageCondition_Flatten\", units=image_shape[0]*image_shape[1], activation=activation)(condition)\n",
        "    condition_image = layers.Reshape(name=\"ImageCondition\", target_shape=(image_shape[0:2] + [1]))(condition_image)\n",
        "\n",
        "    x = layers.Concatenate(name=\"E_Input\", axis=3)([original_image, condition_image])\n",
        "    x = build_encoder_core(activation)(x)\n",
        "\n",
        "    mean = layers.Dense(name='Mean', units=code_size, activation=latent_activation)(x)\n",
        "    log_variance = layers.Dense(name='LogVariance', units=code_size, activation=latent_activation)(x)\n",
        "\n",
        "    return keras.Model(name=\"CVAE_Encoder\", inputs=[original_image, condition], outputs=[mean, log_variance])"
      ],
      "metadata": {
        "id": "0YOCSqm_yLkH"
      },
      "id": "0YOCSqm_yLkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we create an instance of the **Encoder**."
      ],
      "metadata": {
        "id": "HfHPxqDeyGLQ"
      },
      "id": "HfHPxqDeyGLQ"
    },
    {
      "cell_type": "code",
      "source": [
        "cvae_encoder = build_cvae_encoder(\n",
        "    image_shape=image_size,\n",
        "    condition_size=condition_size,\n",
        "    code_size=code_size,\n",
        "    activation=cnn_activation,\n",
        "    latent_activation=latent_activation\n",
        ")"
      ],
      "metadata": {
        "id": "VU8gIpXTyDo0"
      },
      "id": "VU8gIpXTyDo0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the shape of the features extracted from the original images by the **Encoder**."
      ],
      "metadata": {
        "id": "qojVT6PrHA8H"
      },
      "id": "qojVT6PrHA8H"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_shape = cvae_encoder.get_layer(\"Encoder_Core\").get_layer(\"OriginalImage_Features\").input.shape[1:]\n",
        "log(\"Feature shape:\", feature_shape)\n",
        "log(\"Feature count:\", feature_shape.num_elements())"
      ],
      "metadata": {
        "id": "eEM1OeeeHJva"
      },
      "id": "eEM1OeeeHJva",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's take a look at its description."
      ],
      "metadata": {
        "id": "cqFRH2cG0CZd"
      },
      "id": "cqFRH2cG0CZd"
    },
    {
      "cell_type": "code",
      "source": [
        "cvae_encoder.summary()"
      ],
      "metadata": {
        "id": "c13gwH720JUa"
      },
      "id": "c13gwH720JUa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's take a look at its **computation graph**.\n",
        "\n",
        "> _**Note**: there may be some visual bugs in the graph (known issue of\n",
        "`plot_model` when used in composite models with multiple inputs and/or outputs)._"
      ],
      "metadata": {
        "id": "gemncTUaynrJ"
      },
      "id": "gemncTUaynrJ"
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(cvae_encoder, show_shapes=True, show_layer_names=True, expand_nested=True, to_file=directories[\"images\"]+\"cvae_encoder.png\")"
      ],
      "metadata": {
        "id": "1W2ZY6sg0Wh2"
      },
      "id": "1W2ZY6sg0Wh2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "\n",
        "Once the **Encoder** has been defined, the only missing piece of the **CVAE**\n",
        "is the **Decoder**.\n",
        "\n",
        "First, let's define the **Decoder Core**, which is the reverse architecture of the **Encoder Core**."
      ],
      "metadata": {
        "id": "PNGZZkzSP-_h"
      },
      "id": "PNGZZkzSP-_h"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_decoder_core(feature_shape, activation, output_activation):\n",
        "    return keras.Sequential(\n",
        "        name=\"Decoder_Core\",\n",
        "        layers=[\n",
        "            layers.Reshape(name=\"DC_Reshape\", target_shape=feature_shape),\n",
        "            layers.Conv2DTranspose(name=\"DC_TranConvolution1\", filters=128, kernel_size=(2, 2), strides=1, padding='same', activation=activation),\n",
        "            layers.Conv2DTranspose(name=\"DC_TranConvolution2\", filters=64, kernel_size=(2, 2), strides=2, padding='same', activation=activation),\n",
        "            layers.Conv2DTranspose(name=\"DC_TranConvolution3\", filters=32, kernel_size=(2, 2), strides=1, padding='same', activation=activation),\n",
        "            layers.Conv2DTranspose(name=\"DC_TranConvolution4\", filters=16, kernel_size=(2, 2), strides=1, padding='same', activation=activation),\n",
        "            layers.Conv2D(name=\"GeneratedImage\", filters=3, kernel_size=(2, 2), strides=1, padding='same', activation=output_activation),\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "nBbWFGWiQB1k"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nBbWFGWiQB1k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's define the **Decoder**, which contains the **Decoder Core**."
      ],
      "metadata": {
        "id": "qkBplGBHgQJi"
      },
      "id": "qkBplGBHgQJi"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cvae_decoder(feature_shape, code_size, condition_size, activation, output_activation):\n",
        "    code_input = layers.Input(name=\"Code\", shape=code_size)\n",
        "    condition_input = layers.Input(name=\"Condition\", shape=condition_size)\n",
        "\n",
        "    x = layers.Concatenate(name='D_Input')([code_input, condition_input])\n",
        "    x = layers.Dense(name='GeneratedImage_Features', units=feature_shape.num_elements(), activation=activation)(x)\n",
        "    x = build_decoder_core(feature_shape, activation, output_activation)(x)\n",
        "\n",
        "    return keras.Model(name='CVAE_Decoder', inputs=[code_input, condition_input], outputs=x)"
      ],
      "metadata": {
        "id": "eTc59hg2yOy3"
      },
      "id": "eTc59hg2yOy3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we create an instance of the **Decoder**."
      ],
      "metadata": {
        "id": "A8PJ-3k5zKQO"
      },
      "id": "A8PJ-3k5zKQO"
    },
    {
      "cell_type": "code",
      "source": [
        "cvae_decoder = build_cvae_decoder(\n",
        "    feature_shape=feature_shape,\n",
        "    code_size=code_size,\n",
        "    condition_size=condition_size,\n",
        "    activation=cnn_activation,\n",
        "    output_activation=output_activation\n",
        ")"
      ],
      "metadata": {
        "id": "QyBVBnAVy98P"
      },
      "id": "QyBVBnAVy98P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at its description."
      ],
      "metadata": {
        "id": "v8xqAHWjzTXX"
      },
      "id": "v8xqAHWjzTXX"
    },
    {
      "cell_type": "code",
      "source": [
        "cvae_decoder.summary()"
      ],
      "metadata": {
        "id": "TPW9ROeyzTXX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TPW9ROeyzTXX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's take a look at its **computation graph**."
      ],
      "metadata": {
        "id": "8Op8j1U_zTXX"
      },
      "id": "8Op8j1U_zTXX"
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(cvae_decoder, show_shapes=True, show_layer_names=True, expand_nested=True, to_file=directories[\"images\"]+\"cvae_decoder.png\")"
      ],
      "metadata": {
        "id": "6eNdRLEFzTXY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6eNdRLEFzTXY"
    },
    {
      "cell_type": "markdown",
      "id": "5e1464d8-391c-4b99-86c2-6c9f03067c01",
      "metadata": {
        "id": "5e1464d8-391c-4b99-86c2-6c9f03067c01"
      },
      "source": [
        "### CVAE\n",
        "\n",
        "Finally, let's define the **CVAE (Conditional Variational AutoEncoder)** that will contain both the **Encoder** and the **Decoder**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CVAE:\n",
        "    \"\"\"A Conditional Variational AutoEncoder made of the specified 'encoder' and 'decoder'.\"\"\"\n",
        "    def __init__(self, encoder, decoder):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        self.input = self.encoder.input\n",
        "        self.condition_size = self.input[1].shape[1]\n",
        "        self.mean = self.encoder.output[0]\n",
        "        self.log_variance = self.encoder.output[1]\n",
        "        self.code_size = self.mean.shape[1]\n",
        "        self.sampling = MultinormalSamplingLayer(name='Sampling')([self.mean, self.log_variance])\n",
        "        self.output = self.decoder([self.sampling, self.encoder.input[1]])\n",
        "\n",
        "        self.model = keras.Model(name='CVAE', inputs=self.input, outputs=self.output)\n",
        "\n",
        "    def generate_single_data(self, code, data_type):\n",
        "        \"\"\"Generate a new image corresponding to the specified 'code' and 'data_type'.\"\"\"\n",
        "        code_batch = tf.reshape(code, shape=(1, self.code_size))\n",
        "        data_type_batch = tf.reshape(data_type, shape=(1, self.condition_size))\n",
        "        return self.generate_batch_data(code_batch, data_type_batch)[0]\n",
        "\n",
        "    def generate_batch_data(self, code_batch, data_type_batch):\n",
        "        \"\"\"Generate some new images corresponding to the specified batch of codes and data types.\"\"\"\n",
        "        return self.decoder([code_batch, data_type_batch], training=False)"
      ],
      "metadata": {
        "id": "BpirFpTEoapo"
      },
      "id": "BpirFpTEoapo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, an instance of the **CVAE** is created."
      ],
      "metadata": {
        "id": "ZutV00J0L372"
      },
      "id": "ZutV00J0L372"
    },
    {
      "cell_type": "code",
      "source": [
        "cvae = CVAE(cvae_encoder, cvae_decoder)"
      ],
      "metadata": {
        "id": "uDsFRU9iqSlf"
      },
      "id": "uDsFRU9iqSlf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at a description of the model."
      ],
      "metadata": {
        "id": "FiwyH-xEM7uX"
      },
      "id": "FiwyH-xEM7uX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da635b2-b821-487b-9d6f-9124181d9b72",
      "metadata": {
        "id": "4da635b2-b821-487b-9d6f-9124181d9b72"
      },
      "outputs": [],
      "source": [
        "cvae.model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, let's look at the **computation graph** of the model."
      ],
      "metadata": {
        "id": "Lm3oy1W2NZQW"
      },
      "id": "Lm3oy1W2NZQW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e16e50-fedb-40c7-b1e3-d673c78c2949",
      "metadata": {
        "id": "f6e16e50-fedb-40c7-b1e3-d673c78c2949"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(cvae.model, show_shapes=True, show_layer_names=True, expand_nested=True, to_file=directories[\"images\"]+\"cvae.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the user can decide to load the weights learned from previous runs of this notebook into the model."
      ],
      "metadata": {
        "id": "_tiahy2HNjbO"
      },
      "id": "_tiahy2HNjbO"
    },
    {
      "cell_type": "code",
      "source": [
        "load_previous_weights=boolean_input('Load previous weights?')\n",
        "if load_previous_weights:\n",
        "    cvae.model.set_weights(keras.models.load_model(directories[\"model\"]+'cvae.keras', safe_mode=False).get_weights())\n",
        "    log(\"Using pre-trained model.\")\n",
        "else:\n",
        "    log(\"Using new model.\")"
      ],
      "metadata": {
        "id": "dsWcItU7vM-y"
      },
      "id": "dsWcItU7vM-y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d2981c7e-7e03-4999-8085-0af7cf83f197",
      "metadata": {
        "id": "d2981c7e-7e03-4999-8085-0af7cf83f197"
      },
      "source": [
        "---\n",
        "\n",
        "## Training\n",
        "\n",
        "In this section, the training of the previously defined model will be configured\n",
        "and performed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92d771a2-7408-4614-89e4-b0b9e16d167e",
      "metadata": {
        "id": "92d771a2-7408-4614-89e4-b0b9e16d167e"
      },
      "source": [
        "### Configuration\n",
        "\n",
        "Below, a list of the **hyperparameters** that the user can set to affect the\n",
        "**training** of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f09176f-94b5-4e22-bad1-16c7a8b2ff39",
      "metadata": {
        "id": "7f09176f-94b5-4e22-bad1-16c7a8b2ff39"
      },
      "outputs": [],
      "source": [
        "shuffle_buffer_size = 1024                                          # How much shuffling is applied to the data during training (at the cost of memory)\n",
        "batch_size = 16                                                     # How many samples are observed by the model before updating its weights\n",
        "epoch_count = 1000                                                  # How many times the training set will be fully observed by the model\n",
        "patience = 200                                                      # How many epochs of non-improvement before the training is stopped\n",
        "recon_coefficient = 30                                              # How much reconstruction of the original images weights on the loss of the model\n",
        "kl_coefficient = 1                                                  # How much regularization of the latent space weights on the loss of the model\n",
        "learning_rate = 0.001                                               # How much the model will learn from new observations at each training iteration\n",
        "def optimizer(): return tf.keras.optimizers.Adam(learning_rate)     # How the weights of the model are updated during training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Preparation\n",
        "\n",
        "Before training the model, it is necessary to prepare the dataset appropriately\n",
        "for the training session.\n",
        "\n",
        "In particular, when using **tf.data.Dataset**s as inputs, the [fit](https://keras.io/api/models/model_training_apis/#fit-method) method of **Keras**\n",
        "for training a model expects a **tf.data.Dataset** whose elements are mini-batches containing pairs of **(Input, TargetOutput)**.\n",
        "\n",
        "For training the **CVAE**, we'll define a custom loss, so we can avoid specifying the **TargetOutput** in the `fit` method. In other words, each mini-batch of the training set will actually contain singleton tuples of **(Input, )**.\n",
        "\n",
        "Finally, the inputs of a **CVAE** are both the **Input** and the **TargetOutput**, so each mini-batch of the training set will actually contain singleton tuples of **((Input, TargetOutput), )**.\n",
        "\n",
        "This transformation is performed by `to_cvae_input` as shown below."
      ],
      "metadata": {
        "id": "q9V7bKqPyhpQ"
      },
      "id": "q9V7bKqPyhpQ"
    },
    {
      "cell_type": "code",
      "source": [
        "def to_cvae_input(xi, yi): return ((xi, yi),)"
      ],
      "metadata": {
        "id": "SOLo3p_7y0Hb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "SOLo3p_7y0Hb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, the training set is mapped to an input suitable for training a **CVAE**,\n",
        "then it is **cached**, **shuffled** and **batched** using the configuration specified previously.\n",
        "\n",
        "Caching the training set will allow for faster training. In fact, it will store the results of previous computations of the training set pipeline in a file, fetching those results instead of recomputing the pipeline for each object of the training set each time it is observed. In this case, it will avoid recomputing the preprocessing and augmentation pipelines at the start of every epoch.\n",
        "\n",
        "Instead, the `shuffle` and `batch` pipelines are added after caching, so they\n",
        "will be recomputed at the start of every epoch.\n",
        "\n",
        "> _**Note**: `prefetch` is just a **Tensorflow** optimization on the generation\n",
        "of the objects of a dataset._"
      ],
      "metadata": {
        "id": "9T24H_uiTGZF"
      },
      "id": "9T24H_uiTGZF"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training_set(training_set, prepare_fn, cache_name):\n",
        "    training_set = training_set.map(prepare_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    training_set = training_set.cache(filename = directories[\"cache\"] + cache_name)\n",
        "    training_set = training_set.shuffle(buffer_size = shuffle_buffer_size)\n",
        "    training_set = training_set.batch(batch_size = batch_size)\n",
        "    training_set = training_set.prefetch(tf.data.AUTOTUNE)\n",
        "    return training_set"
      ],
      "metadata": {
        "id": "HjsxXSSdzNnt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "HjsxXSSdzNnt"
    },
    {
      "cell_type": "code",
      "source": [
        "cvae_training_set = prepare_training_set(training_set, prepare_fn=to_cvae_input, cache_name=\"cvae_training_set\")"
      ],
      "metadata": {
        "id": "XnWl3SOQ9GO5"
      },
      "id": "XnWl3SOQ9GO5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar transformations must also be applied to the validation and test sets."
      ],
      "metadata": {
        "id": "0GpNKWrsUpx1"
      },
      "id": "0GpNKWrsUpx1"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_validation_set(validation_set, prepare_fn, cache_name):\n",
        "    validation_set = validation_set.map(prepare_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    validation_set = validation_set.batch(batch_size = batch_size)\n",
        "    validation_set = validation_set.cache(filename = directories[\"cache\"] + cache_name)\n",
        "    validation_set = validation_set.prefetch(tf.data.AUTOTUNE)\n",
        "    return validation_set"
      ],
      "metadata": {
        "id": "gm0LP0Xd9T0a"
      },
      "id": "gm0LP0Xd9T0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The preparation of the test set is the same as the preparation of the validation set\n",
        "prepare_test_set = prepare_validation_set"
      ],
      "metadata": {
        "id": "5X1FCvaDyHn8"
      },
      "id": "5X1FCvaDyHn8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvae_validation_set = prepare_validation_set(validation_set, prepare_fn=to_cvae_input, cache_name=\"cvae_validation_set\")\n",
        "cvae_test_set = prepare_test_set(test_set, prepare_fn=to_cvae_input, cache_name=\"cvae_test_set\")"
      ],
      "metadata": {
        "id": "gTeVUSaqzLqd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gTeVUSaqzLqd"
    },
    {
      "cell_type": "markdown",
      "id": "8df88186-458a-496d-9221-2246a6053dd1",
      "metadata": {
        "id": "8df88186-458a-496d-9221-2246a6053dd1"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "In order to train a model, it is necessary to define a **loss function**, which\n",
        "tells what behavior should be learned by the model.\n",
        "\n",
        "As discussed in the introduction, the **loss function** for this model will\n",
        "consider the **Reconstruction Error** and the **Regularization Error**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reconstruction Error\n",
        "\n",
        "The **Reconstruction Error** $E_{recon}$ is evaluated as the **loss** between the original images $x$ and the generated images $x'$.\n",
        "\n",
        "Such **loss** can be evaluated as the **cost** between the original images and\n",
        "the generated images multiplied by the number of images observed by the model\n",
        "every epoch.\n",
        "\n",
        "In this case, the **cost** chosen for the **Reconstruction Error** is the\n",
        "**Mean Squared Error (MSE)** and the corresponding **loss** is the **L2 Loss**.\n",
        "In particular, the **Reconstruction Error** is evaluated as follows:\n",
        "\n",
        "$\n",
        "E_{recon}\n",
        "= MSE(x, x') \\cdot |T|\n",
        "= \\frac{\\sum_{i=1}^{|T|} (x_{i} - x_{i}')^2}{|T|} \\cdot |T|\n",
        "= \\sum_{i=1}^{|T|} (x_{i} - x_{i}')^2\n",
        "= L_2(x, x')\n",
        "$\n",
        "\n",
        "where\n",
        "\n",
        "$T$ : is the training set\n",
        "\n",
        "$x_i$ : is the $i^{th}$ image of the training set\n",
        "\n",
        "$x_i'$ : is the image generated by the model when receiving $x_i$ as input\n",
        "\n",
        "The purpose of the **Reconstruction Error** is to train the model to generate\n",
        "images that are faithful with respect to the original images of the training set."
      ],
      "metadata": {
        "id": "-3wlHk4K-7sA"
      },
      "id": "-3wlHk4K-7sA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500274b5-a54a-4a43-ac60-36a629daa8ba",
      "metadata": {
        "id": "500274b5-a54a-4a43-ac60-36a629daa8ba"
      },
      "outputs": [],
      "source": [
        "def reconstruction_error(input, output, training_set_size):\n",
        "    input_flatten = tf.reshape(input, shape=(-1, tf.size(input)))\n",
        "    output_flatten = tf.reshape(output, shape=(-1, tf.size(output)))\n",
        "    return keras.losses.mean_squared_error(input_flatten, output_flatten) * training_set_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cvae_recon_error = reconstruction_error(cvae.model.input[0], cvae.model.output, training_set_size)"
      ],
      "metadata": {
        "id": "0jTE2H0e8_z6"
      },
      "id": "0jTE2H0e8_z6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regularization Error\n",
        "\n",
        "The **Regularization Error** $E_{regul}$ is evaluated as the **KL Divergence**\n",
        "between the latent space distributions generated by the **Encoder** and the standard multinormal distribution. In fact, the **KL Divergence** is a measure of the difference between two probability distributions, meaning that the model\n",
        "will be trained to produce distributions in the latent space that are as close\n",
        "as possible to a standard multinormal distribution.\n",
        "\n",
        "In particular, the **Regularization Error** is evaluated as follows:\n",
        "\n",
        "\\begin{align}\n",
        "E_{regul}\n",
        "&=  * KL[N(,) || N(0^{k}, I_{k})] = \\\\\n",
        "&=  * \\frac{1}{2}[(0^k-)^T \\cdot I_{k}^{-1} \\cdot (0^k-) + tr(I_{k}^{-1} \\cdot ) + ln\\frac{det(I_{k})}{det()} - k] = \\\\\n",
        "&=  * \\frac{1}{2}[()^T + tr() - ln(det()) - k]\n",
        "\\end{align}\n",
        "\n",
        "where\n",
        "\n",
        "$$ : is the **KL Coefficient** (hyperparameter).\n",
        "\n",
        "$k$ : is the number of dimensions of the latent space.\n",
        "\n",
        "> _**Note**: if we consider the **logaritmic variance** $ln(^2)$ instead of the **covariance matrix** $$, then $E_{regul}$ can be computed using the following formula: \\\\\n",
        "$E_{regul} =  \\cdot \\frac{1}{2} \\sum_{i=1}^{k} [_i^2 + e^{ln(_i^2)} - ln(_i^2) - 1]$_\n",
        "\n",
        "The purpose of the **Regularization Error** is to train the **Encoder** to\n",
        "generate a **regular** latent space, that is both **complete** and **continuous**, so that the latent space could be safely sampled for generating\n",
        "new data.\n",
        "\n"
      ],
      "metadata": {
        "id": "E2bX_bAV-_8G"
      },
      "id": "E2bX_bAV-_8G"
    },
    {
      "cell_type": "code",
      "source": [
        "def regularization_error(mean, log_variance, kl_coefficient):\n",
        "    return kl_coefficient * 0.5 * (K.sum(K.square(mean) + K.exp(log_variance) - log_variance - 1, axis = -1))"
      ],
      "metadata": {
        "id": "KexY-oScOLKa"
      },
      "id": "KexY-oScOLKa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvae_regul_error = regularization_error(cvae.mean, cvae.log_variance, kl_coefficient)"
      ],
      "metadata": {
        "id": "xQ1N-rPO8uV1"
      },
      "id": "xQ1N-rPO8uV1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cumulative Loss\n",
        "\n",
        "Finally, the cumulative loss of the model is the sum of the **Reconstruction Error** and the **Regularization Error**, as defined below."
      ],
      "metadata": {
        "id": "H4-ImPc7OMjF"
      },
      "id": "H4-ImPc7OMjF"
    },
    {
      "cell_type": "code",
      "source": [
        "def cvae_loss(recon_error, regul_error, recon_coefficient): return recon_coefficient * recon_error + regul_error"
      ],
      "metadata": {
        "id": "iWl3n5anORG-"
      },
      "id": "iWl3n5anORG-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once a loss has been defined, it is necessary to tell **Keras** to use that loss\n",
        "during training, using the method `add_loss` (for losses that do not depend only on\n",
        "the **Input** and **TargetOutput**, like in this case)."
      ],
      "metadata": {
        "id": "vXMNcTA8FF_X"
      },
      "id": "vXMNcTA8FF_X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "911f6142-192c-4753-9792-e006678aaa88",
      "metadata": {
        "id": "911f6142-192c-4753-9792-e006678aaa88"
      },
      "outputs": [],
      "source": [
        "cvae.model.add_loss(cvae_loss(cvae_recon_error, cvae_regul_error, recon_coefficient))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even if the loss that the **CVAE** should minimize is the sum of the **Reconstruction Error** and **Regularization Error**, it is still useful to\n",
        "monitor both errors separately as metrics, in order to understand why the model is struggling to achieve better performances."
      ],
      "metadata": {
        "id": "xICpy7z-72BU"
      },
      "id": "xICpy7z-72BU"
    },
    {
      "cell_type": "code",
      "source": [
        "cvae.model.add_metric(cvae_recon_error, name=\"reconstruction_error\")\n",
        "cvae.model.add_metric(cvae_regul_error, name=\"regularization_error\")"
      ],
      "metadata": {
        "id": "xhM82H998sAl"
      },
      "id": "xhM82H998sAl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "df8d0e18-fa0c-45d0-b230-76a3eb013d18",
      "metadata": {
        "id": "df8d0e18-fa0c-45d0-b230-76a3eb013d18"
      },
      "source": [
        "### Callbacks\n",
        "\n",
        "In this subsection, we'll defined a set of callbacks to execute during the\n",
        "training of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63745f09-60c4-46e0-9ca2-3e45690e26a4",
      "metadata": {
        "id": "63745f09-60c4-46e0-9ca2-3e45690e26a4"
      },
      "source": [
        "#### Image Generation Callback\n",
        "\n",
        "In order to monitor the images generated by the **CVAE** during the training, here a callback is provided to generate a batch of images at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6050728-4361-443c-8830-710d2ecf9a83",
      "metadata": {
        "id": "f6050728-4361-443c-8830-710d2ecf9a83"
      },
      "outputs": [],
      "source": [
        "class ImageGenerationCallback(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    A callback that generate an image for each possible combination of the\n",
        "    specified 'image_types' and 'latent_points' at the end of each epoch,\n",
        "    using the specified 'cvae'.\n",
        "    If a number of 'random_latent_points' greater than zero is specified,\n",
        "    the callback will also generate the images corresponding to those\n",
        "    points (randomly generated at the end of each epoch) for each of the\n",
        "    'image_types'.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "        cvae,\n",
        "        image_types = tf.one_hot(tf.constant(list(label_names.keys()), shape=(label_count, )), label_count),\n",
        "        latent_points = [tf.zeros(shape=(code_size,))],\n",
        "        random_latent_points = 0,\n",
        "        generateWhen = lambda epoch: True\n",
        "    ):\n",
        "        keras.callbacks.Callback.__init__(self)\n",
        "        self.cvae = cvae\n",
        "        self.image_types = image_types\n",
        "        self.latent_points = latent_points\n",
        "        self.generateWhen = generateWhen\n",
        "        self.random_latent_point_count = random_latent_points\n",
        "        self.sample_dataset = self.__combinations(self.image_types, self.latent_points)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        actualEpoch = epoch + 1\n",
        "        if self.generateWhen(actualEpoch):\n",
        "            print()\n",
        "            log(\"Generating fixed images for epoch\", actualEpoch, \"...\")\n",
        "            generated_image_dataset = self.sample_dataset.map(lambda x,y: (self.cvae.generate_single_data(x, y), y))\n",
        "            show_dataset_sample(\n",
        "                dataset=generated_image_dataset,\n",
        "                n_rows=len(self.latent_points),\n",
        "                n_cols=len(self.image_types),\n",
        "                show_label_probability=False,\n",
        "            )\n",
        "\n",
        "            if self.random_latent_point_count > 0 :\n",
        "                log(\"Generating random images for epoch\", actualEpoch, \"...\")\n",
        "                random_latent_points = tf.random.normal(shape=(self.random_latent_point_count, self.cvae.code_size))\n",
        "                random_sample_dataset = self.__combinations(self.image_types, random_latent_points)\n",
        "                generated_image_dataset = random_sample_dataset.map(lambda x,y: (self.cvae.generate_single_data(x, y), y))\n",
        "                show_dataset_sample(\n",
        "                    dataset=generated_image_dataset,\n",
        "                    n_rows=len(random_latent_points),\n",
        "                    n_cols=len(self.image_types),\n",
        "                    show_label_probability=False,\n",
        "                )\n",
        "\n",
        "    def __combinations(self, image_types, latent_points):\n",
        "        \"\"\"Return a dataset containing all the possible combinations between the specified 'image_types' and 'latent_points'\"\"\"\n",
        "        samples_x, samples_y = [], []\n",
        "        for latent_point in latent_points:\n",
        "            for image_type in image_types:\n",
        "                samples_x.append(latent_point)\n",
        "                samples_y.append(image_type)\n",
        "        return tf.data.Dataset.from_tensor_slices((samples_x, samples_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e8f1b3-af41-4b24-b73d-19c05da7797a",
      "metadata": {
        "id": "b4e8f1b3-af41-4b24-b73d-19c05da7797a"
      },
      "source": [
        "### Compilation\n",
        "\n",
        "The last step before training is to compile the model, choosing the type of\n",
        "training algorithm that will be used for training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578330e4-e3f0-4a1f-89fd-87777c65dd6b",
      "metadata": {
        "id": "578330e4-e3f0-4a1f-89fd-87777c65dd6b"
      },
      "outputs": [],
      "source": [
        "cvae.model.compile(optimizer=optimizer())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86ec9ca3-e39f-48aa-b00b-2b655993bab3",
      "metadata": {
        "id": "86ec9ca3-e39f-48aa-b00b-2b655993bab3"
      },
      "source": [
        "### Execution\n",
        "\n",
        "Finally, let's train the model using the prepared training and validation sets.\n",
        "\n",
        "During training, the following callbacks will be used:\n",
        "- [Early Stopping](https://keras.io/api/callbacks/early_stopping/): stops the\n",
        "training session after `patience` epochs of non-improvement of a monitored metric.\n",
        "In this case, it will monitor the loss evaluated on the validation set.\n",
        "- [Model Checkpoint](https://keras.io/api/callbacks/model_checkpoint/): saves\n",
        "the current state of the model at the end of each epoch during the training session,\n",
        "in order to avoid losing progress after an eventual crash of the system.\n",
        "- **Image Generation Callback**: generates a batch of images using the model\n",
        "at the end of each epoch. In particular, the callback has been configured to\n",
        "generate the images corresponding to the origin of the latent space and some random sample points for each landscape type at the end of each epoch. \\\\\n",
        "The origin of the latent space should correspond to the mean of the distribution\n",
        "in the latent space discovered by the encoder (due to the regularization error). In that case, it represents the landscape representation that is closest to all the landscape representations learned by the model. In other words, its the **most general representation** that the model was able to find **for each landscape type**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d8dc71-054b-4a16-b506-63a2a0703939",
      "metadata": {
        "id": "a1d8dc71-054b-4a16-b506-63a2a0703939"
      },
      "outputs": [],
      "source": [
        "cvae_history = cvae.model.fit(\n",
        "    cvae_training_set,\n",
        "    validation_data=cvae_validation_set,\n",
        "    epochs=epoch_count,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor='loss', patience=patience, restore_best_weights=True),\n",
        "        keras.callbacks.ModelCheckpoint(monitor='loss', filepath=directories[\"model\"]+'cvae.checkpoint.keras'),\n",
        "        ImageGenerationCallback(cvae, random_latent_points=4, generateWhen=lambda epoch: epoch%10==1 or epoch==epoch_count),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the history of the loss and the other metrics on the training and\n",
        "validation sets."
      ],
      "metadata": {
        "id": "Q658sDvuKZsJ"
      },
      "id": "Q658sDvuKZsJ"
    },
    {
      "cell_type": "code",
      "source": [
        "history_metrics = [\n",
        "    'reconstruction_error',\n",
        "    'regularization_error',\n",
        "    'val_loss',\n",
        "    'val_reconstruction_error',\n",
        "    'val_regularization_error'\n",
        "]"
      ],
      "metadata": {
        "id": "1aMiP8_8rnJe"
      },
      "id": "1aMiP8_8rnJe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f33d62d-a5d3-40a6-aca6-c39f6c976bcc",
      "metadata": {
        "id": "5f33d62d-a5d3-40a6-aca6-c39f6c976bcc"
      },
      "outputs": [],
      "source": [
        "plot_history(cvae_history, metrics=history_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By observing the following graph it is possible to infer if the model is\n",
        "**underfitting**, meaning that it's too simple to perform well on the task, or **overfitting**, meaning that it's learning too much from the training set and\n",
        "it does not generalize enough to perform well on new data.\n",
        "\n",
        "For an **AutoEncoder**, **overfitting** would be the desired outcome, even\n",
        "though being able to reconstruct never-observed images like those of the\n",
        "validation set would still be good, meaning that the model has learned a\n",
        "more general representation of the data it is trained to generate."
      ],
      "metadata": {
        "id": "1oH3DgvzPlQI"
      },
      "id": "1oH3DgvzPlQI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the last step of training, let's save the model, so that its weights may be\n",
        "loaded in the future, for using the model or continuing its training."
      ],
      "metadata": {
        "id": "IyJ2V3EKNP_C"
      },
      "id": "IyJ2V3EKNP_C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d4eb49-6ff9-4df9-ac0e-378843053f9d",
      "metadata": {
        "id": "44d4eb49-6ff9-4df9-ac0e-378843053f9d"
      },
      "outputs": [],
      "source": [
        "cvae.model.save(directories[\"model\"]+'cvae.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "`[11-22-2023]`\n",
        "\n",
        "In this revision of the notebook, from the training curve of the **CVAE**, we can observe that there is still a margin for improvement on the training set by increasing the number of training epochs.\n",
        "\n",
        "Another consideration is that the model is overfitting. In fact, the training and validation losses are diverging."
      ],
      "metadata": {
        "id": "frssEZ-dNvfE"
      },
      "id": "frssEZ-dNvfE"
    },
    {
      "cell_type": "markdown",
      "id": "e1d9a417-33cd-4ccd-a991-14c5a8c5f45b",
      "metadata": {
        "id": "e1d9a417-33cd-4ccd-a991-14c5a8c5f45b"
      },
      "source": [
        "---\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "In this section, the performance of the trained **Landscape Generator** will be measured, using the metrics discussed during the introduction of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration\n",
        "\n",
        "Below, the user can configure some parameters for the validation."
      ],
      "metadata": {
        "id": "B-imdfaXNvxh"
      },
      "id": "B-imdfaXNvxh"
    },
    {
      "cell_type": "code",
      "source": [
        "samples_per_test_set = 128   # The number of samples per landscape type to generate and evaluate (requires gpu memory)"
      ],
      "metadata": {
        "id": "WTTdNk4SNxjQ"
      },
      "id": "WTTdNk4SNxjQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Landscape\n",
        "\n",
        "First, let's define a utility class for generating datasets of images representing a specific landscape type using the trained model."
      ],
      "metadata": {
        "id": "3ofrabkmmeEr"
      },
      "id": "3ofrabkmmeEr"
    },
    {
      "cell_type": "code",
      "source": [
        "class LandscapeGenerator:\n",
        "    \"\"\"A landscape generator relying on the specified 'cvae' for the generation of landscapes.\"\"\"\n",
        "    def __init__(self, cvae):\n",
        "      self.cvae = cvae\n",
        "\n",
        "    def generate(self, landscape_type, samples = 100):\n",
        "        \"\"\"Generate the specified number of 'samples' of the specified 'landscape_type'.\"\"\"\n",
        "        label_id = label_id_of(landscape_type)\n",
        "        latent_points = tf.random.normal(shape=(samples, self.cvae.code_size))\n",
        "        image_types = keras.utils.to_categorical(tf.repeat([label_id], samples), num_classes=self.cvae.condition_size)\n",
        "        return tf.data.Dataset.from_tensor_slices((self.cvae.generate_batch_data(latent_points, image_types), image_types))"
      ],
      "metadata": {
        "id": "Yxx1Lg_QR63L"
      },
      "id": "Yxx1Lg_QR63L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "landscape_generator = LandscapeGenerator(cvae)"
      ],
      "metadata": {
        "id": "yLpgJwa6TLwQ"
      },
      "id": "yLpgJwa6TLwQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Landscape Generation\n",
        "\n",
        "Then, let's create a test set for each landscape type, each containing generated\n",
        "images of the corresponding landscape."
      ],
      "metadata": {
        "id": "N065B3gdnFR0"
      },
      "id": "N065B3gdnFR0"
    },
    {
      "cell_type": "code",
      "source": [
        "coast_test_set = landscape_generator.generate(\"Coast\", samples=samples_per_test_set)\n",
        "desert_test_set = landscape_generator.generate(\"Desert\", samples=samples_per_test_set)\n",
        "forest_test_set = landscape_generator.generate(\"Forest\", samples=samples_per_test_set)\n",
        "glacier_test_set = landscape_generator.generate(\"Glacier\", samples=samples_per_test_set)\n",
        "mountain_test_set = landscape_generator.generate(\"Mountain\", samples=samples_per_test_set)"
      ],
      "metadata": {
        "id": "i-nCH7DO0dsx"
      },
      "id": "i-nCH7DO0dsx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generated Landscape Visualization\n",
        "\n",
        "Let's take a look at a sample of generated images for each landscape type."
      ],
      "metadata": {
        "id": "0sRmOJPgoD4f"
      },
      "id": "0sRmOJPgoD4f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Coasts"
      ],
      "metadata": {
        "id": "aKZka4SRoduS"
      },
      "id": "aKZka4SRoduS"
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(coast_test_set, n_cols=5, n_rows=5, image_header=False)"
      ],
      "metadata": {
        "id": "oms3gQT7oHW-"
      },
      "id": "oms3gQT7oHW-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deserts"
      ],
      "metadata": {
        "id": "S5U9LNdgof2b"
      },
      "id": "S5U9LNdgof2b"
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(desert_test_set, n_cols=5, n_rows=5, image_header=False)"
      ],
      "metadata": {
        "id": "Vb5TmjgvoW6t"
      },
      "id": "Vb5TmjgvoW6t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forests"
      ],
      "metadata": {
        "id": "oHqyIwq1ojtb"
      },
      "id": "oHqyIwq1ojtb"
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(forest_test_set, n_cols=5, n_rows=5, image_header=False)"
      ],
      "metadata": {
        "id": "vecK5f7UoW1b"
      },
      "id": "vecK5f7UoW1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Glaciers"
      ],
      "metadata": {
        "id": "lwYQ18XGolrx"
      },
      "id": "lwYQ18XGolrx"
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(glacier_test_set, n_cols=5, n_rows=5, image_header=False)"
      ],
      "metadata": {
        "id": "yEdqGxZroWu6"
      },
      "id": "yEdqGxZroWu6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mountains"
      ],
      "metadata": {
        "id": "qCXPGu5Con_f"
      },
      "id": "qCXPGu5Con_f"
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(mountain_test_set, n_cols=5, n_rows=5, image_header=False)"
      ],
      "metadata": {
        "id": "SmD5Q8mXoWlE"
      },
      "id": "SmD5Q8mXoWlE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Evaluation\n",
        "\n",
        "Provided the test sets of generated images, it is possible to evaluate the\n",
        "performance of the model by using the **Loss** and the **Generation Quality** as metrics, just as\n",
        "discussed in the introduction."
      ],
      "metadata": {
        "id": "H8geIwHba1eX"
      },
      "id": "H8geIwHba1eX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Landscape Classifier\n",
        "\n",
        "In order to evaluate the **Generation Quality** of the landscape images generated by\n",
        "the model, we'll load the best classifier trained on the original dataset\n",
        "from Kaggle, already downloaded during the environment configuration."
      ],
      "metadata": {
        "id": "QJ7HYD0Wln7s"
      },
      "id": "QJ7HYD0Wln7s"
    },
    {
      "cell_type": "code",
      "source": [
        "landscape_classifier = keras.models.load_model('./landscape-classifier.h5', custom_objects={'KerasLayer':tfhub.KerasLayer})"
      ],
      "metadata": {
        "id": "eCmpRfQ1s2Gr"
      },
      "id": "eCmpRfQ1s2Gr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at its description."
      ],
      "metadata": {
        "id": "DmoGr3i3m9Ux"
      },
      "id": "DmoGr3i3m9Ux"
    },
    {
      "cell_type": "code",
      "source": [
        "landscape_classifier.summary()"
      ],
      "metadata": {
        "id": "Mz_95PYKs92T"
      },
      "id": "Mz_95PYKs92T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's take a look at its computation graph."
      ],
      "metadata": {
        "id": "tKLtpmiUnB6b"
      },
      "id": "tKLtpmiUnB6b"
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(landscape_classifier, show_shapes=True, show_layer_names=True, expand_nested=True, to_file=directories[\"images\"]+\"landscape-classifier.png\")"
      ],
      "metadata": {
        "id": "mqi8LcqMupZl"
      },
      "id": "mqi8LcqMupZl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From its computation graph,\n",
        "we can infer that the landscape classifier can process batches of 32 images of\n",
        "unknown size at a time."
      ],
      "metadata": {
        "id": "2cQZhYOB-lkW"
      },
      "id": "2cQZhYOB-lkW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's look at the metrics that it will compute during the evaluation\n",
        "of its performance on a dataset."
      ],
      "metadata": {
        "id": "Rm-2w4E1nG2O"
      },
      "id": "Rm-2w4E1nG2O"
    },
    {
      "cell_type": "code",
      "source": [
        "log(\"Landscape Classifier Metrics:\", landscape_classifier.metrics_names)"
      ],
      "metadata": {
        "id": "l4yInFyoAo3h"
      },
      "id": "l4yInFyoAo3h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the metrics that we can use for evaluating the quality of the images\n",
        "generated by the model."
      ],
      "metadata": {
        "id": "B-rvrjS-ooFV"
      },
      "id": "B-rvrjS-ooFV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Landscape Evaluator\n",
        "\n",
        "Before the evaluation of the model, let's define a utility class for evaluating\n",
        "the quality of the landscape images in a dataset."
      ],
      "metadata": {
        "id": "imSdJ1nSP2Xt"
      },
      "id": "imSdJ1nSP2Xt"
    },
    {
      "cell_type": "code",
      "source": [
        "class LandscapeEvaluator:\n",
        "    \"\"\"A landscape evaluator that measure the quality of images representing landscapes.\"\"\"\n",
        "    def __init__(self, landscape_classifier):\n",
        "      self.landscape_classifier = landscape_classifier\n",
        "\n",
        "    def evaluate(self, landscape_dataset):\n",
        "        \"\"\"Evaluate the average quality of the images in the specified 'landscape_dataset'.\"\"\"\n",
        "        return self.landscape_classifier.evaluate(\n",
        "            x=landscape_dataset.map(lambda xi, yi: (xi, tf.argmax(yi))).batch(32),\n",
        "            verbose=1,\n",
        "            return_dict=True,\n",
        "        )"
      ],
      "metadata": {
        "id": "Xub-5ojTP7ei"
      },
      "id": "Xub-5ojTP7ei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "landscape_evaluator = LandscapeEvaluator(landscape_classifier)"
      ],
      "metadata": {
        "id": "ahRWZA_-QhGr"
      },
      "id": "ahRWZA_-QhGr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it on the original dataset. Supposedly, it should have an **accuracy** of **91.83%** on the test set of the original Kaggle competition,\n",
        "so it should achieve similar performances on the whole dataset."
      ],
      "metadata": {
        "id": "B0n69OFFpCWB"
      },
      "id": "B0n69OFFpCWB"
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_performance = landscape_evaluator.evaluate(dataset)\n",
        "log(\"Loss:\", evaluator_performance['loss'])\n",
        "log(\"Accuracy:\", evaluator_performance['accuracy'])"
      ],
      "metadata": {
        "id": "IX86cC0ypFb-"
      },
      "id": "IX86cC0ypFb-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generation Quality\n",
        "\n",
        "Below, the **Landscape Evaluator** is used to measure the **Generation Quality** of each type of landscape generated by the **Landscape Generator** separately."
      ],
      "metadata": {
        "id": "ab5ewMaSvksH"
      },
      "id": "ab5ewMaSvksH"
    },
    {
      "cell_type": "code",
      "source": [
        "coast_evaluation = landscape_evaluator.evaluate(coast_test_set)\n",
        "desert_evaluation = landscape_evaluator.evaluate(desert_test_set)\n",
        "forest_evaluation = landscape_evaluator.evaluate(forest_test_set)\n",
        "glacier_evaluation = landscape_evaluator.evaluate(glacier_test_set)\n",
        "mountain_evaluation = landscape_evaluator.evaluate(mountain_test_set)"
      ],
      "metadata": {
        "id": "B8sSsAjxyT9M"
      },
      "id": "B8sSsAjxyT9M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, an average of the **Generation Quality** of all the landscape types is computed."
      ],
      "metadata": {
        "id": "4KQ3ELvirLjt"
      },
      "id": "4KQ3ELvirLjt"
    },
    {
      "cell_type": "code",
      "source": [
        "evaluations = [coast_evaluation, desert_evaluation, forest_evaluation, glacier_evaluation, mountain_evaluation]\n",
        "avg_evaluation = {\n",
        "    'loss': sum(map(lambda eval: eval['loss'], evaluations)) / len(evaluations),\n",
        "    'accuracy': sum(map(lambda eval: eval['accuracy'], evaluations)) / len(evaluations)\n",
        "}"
      ],
      "metadata": {
        "id": "zDcEph4l1_kZ"
      },
      "id": "zDcEph4l1_kZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reconstruction & Regularization\n",
        "\n",
        "Here, the **Loss** on the training, validation and test sets is computed, as a\n",
        "metric measuring both the **Reconstruction Error** and the **Regularization Error**."
      ],
      "metadata": {
        "id": "UmEHduP_xKwk"
      },
      "id": "UmEHduP_xKwk"
    },
    {
      "cell_type": "code",
      "source": [
        "training_evaluation = cvae.model.evaluate(cvae_training_set, return_dict=True)\n",
        "validation_evaluation = cvae.model.evaluate(cvae_validation_set, return_dict=True)\n",
        "test_evaluation = cvae.model.evaluate(cvae_test_set, return_dict=True)"
      ],
      "metadata": {
        "id": "wEgMQqrqxRY3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wEgMQqrqxRY3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get a better understanding of the reconstruction capabilities of\n",
        "the model, let's try to reconstruct some of the original images.\n",
        "\n",
        "> _**Note**: each original image will have its reconstructed version to its\n",
        "right_."
      ],
      "metadata": {
        "id": "GXhFLJAauae6"
      },
      "id": "GXhFLJAauae6"
    },
    {
      "cell_type": "code",
      "source": [
        "((original_images, original_labels),) = next(iter(cvae_training_set))\n",
        "reconstructed_images = cvae.model.predict([((original_images, original_labels),)])\n",
        "\n",
        "original_images_dataset = tf.data.Dataset.zip(tf.data.Dataset.from_tensor_slices(original_images), tf.data.Dataset.from_tensor_slices(original_labels))\n",
        "reconstructed_images_dataset = tf.data.Dataset.zip(tf.data.Dataset.from_tensor_slices(reconstructed_images), tf.data.Dataset.from_tensor_slices(original_labels))\n",
        "original_vs_reconstructed_dataset = tf.data.Dataset.choose_from_datasets([original_images_dataset, reconstructed_images_dataset], tf.data.Dataset.range(2).repeat())"
      ],
      "metadata": {
        "id": "MqDe1P-murjl"
      },
      "id": "MqDe1P-murjl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(original_vs_reconstructed_dataset, n_rows = 8, n_cols = 4, show_label_probability=False)"
      ],
      "metadata": {
        "id": "yrfDhGD_0KL6"
      },
      "id": "yrfDhGD_0KL6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance Summary\n",
        "\n",
        "Finally, a summary of the configuration of the model and the achieved performances is provided."
      ],
      "metadata": {
        "id": "PeCGCgREbqcY"
      },
      "id": "PeCGCgREbqcY"
    },
    {
      "cell_type": "code",
      "source": [
        "def log_landscape_evaluation(landscape_type, landscape_evaluation):\n",
        "    \"\"\"Print the 'accuracy' and 'loss' obtained on the specified 'landscape_type'\"\"\"\n",
        "    log('{:10s} | accuracy: {:10.2f}% | loss: {:10.3f}'.format(landscape_type, landscape_evaluation['accuracy']*100, landscape_evaluation['loss']))"
      ],
      "metadata": {
        "id": "GZjygVE3cIHK"
      },
      "id": "GZjygVE3cIHK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log('Configuration')\n",
        "log('- Preprocessing:')\n",
        "log('  - Dataset Size:', dataset_size)\n",
        "log('  - Dataset Labels:', label_ids)\n",
        "log('  - Image Size:', next(iter(dataset))[0].shape)\n",
        "log('  - Label Size:', next(iter(dataset))[1].shape)\n",
        "log('- Augmentations:')\n",
        "log('  - Grayscale:', \"Yes\" if use_grayscale_augmentation else \"No\")\n",
        "log('  - Cut Out:', \"Yes\" if use_cut_out_augmentation else \"No\")\n",
        "log('- Architecture:')\n",
        "log('  - Condition Size:', condition_size)\n",
        "log('  - Code Size:', code_size)\n",
        "log('  - CNN Activation:', cnn_activation)\n",
        "log('  - Latent Activation:', latent_activation if latent_activation else \"None\")\n",
        "log('  - Output Activation:', output_activation)\n",
        "log('- Training:')\n",
        "log('  - Training Set Size:', training_set_size)\n",
        "log('  - Validation Size:', validation_set_size)\n",
        "log('  - Test Size:', test_set_size)\n",
        "log('  - Shuffle Size:', shuffle_buffer_size)\n",
        "log('  - Batch Size:', batch_size)\n",
        "log('  - Max Epochs:', epoch_count)\n",
        "log('  - Patience:', patience)\n",
        "log('  - KL Coefficient:', kl_coefficient)\n",
        "log('  - Optimizer:', optimizer())\n",
        "log('  - Learning Rate:', learning_rate)\n",
        "log('  - Pre-trained:', \"Yes\" if load_previous_weights else \"No\")\n",
        "\n",
        "print()\n",
        "\n",
        "log('Reconstruction & Regularization')\n",
        "log('Training loss:   {:.6f}'.format(training_evaluation['loss']))\n",
        "log('- Reconstruction Error:   {:.6f}'.format(training_evaluation['reconstruction_error']))\n",
        "log('- Regularization Error:   {:.6f}'.format(training_evaluation['regularization_error']))\n",
        "log('Validation loss:   {:.6f}'.format(validation_evaluation['loss']))\n",
        "log('- Reconstruction Error:   {:.6f}'.format(validation_evaluation['reconstruction_error']))\n",
        "log('- Regularization Error:   {:.6f}'.format(validation_evaluation['regularization_error']))\n",
        "log('Test loss:   {:.6f}'.format(test_evaluation['loss']))\n",
        "log('- Reconstruction Error:   {:.6f}'.format(test_evaluation['reconstruction_error']))\n",
        "log('- Regularization Error:   {:.6f}'.format(test_evaluation['regularization_error']))\n",
        "\n",
        "print()\n",
        "\n",
        "log('Generation Quality')\n",
        "log_landscape_evaluation('Coast', coast_evaluation)\n",
        "log_landscape_evaluation('Desert', desert_evaluation)\n",
        "log_landscape_evaluation('Forest', forest_evaluation)\n",
        "log_landscape_evaluation('Glacier', glacier_evaluation)\n",
        "log_landscape_evaluation('Mountain', mountain_evaluation)\n",
        "log('-'*53)\n",
        "log_landscape_evaluation('Average', avg_evaluation)"
      ],
      "metadata": {
        "id": "mdJtYlakbs2f"
      },
      "id": "mdJtYlakbs2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "`[11-22-2023]`\n",
        "\n",
        "In this revision of the notebook, we can observe that the **CVAE** is not able\n",
        "to fully reconstruct the details of the original images with a latent space of 200 dimensions after 100 epochs.\n",
        "\n",
        "In the performance summary, we can see the effects of overfitting in the loss\n",
        "achieved on the training, validation and test sets. Also, the **Generation\n",
        "Quality** is pretty low overall, probably because the generated images do not\n",
        "contain many details and they often contain a mix of elements from different landscape types."
      ],
      "metadata": {
        "id": "B3xJj5FmMJWI"
      },
      "id": "B3xJj5FmMJWI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Further Analysis\n",
        "\n",
        "In this section, an analysis of the results of the trained **Landscape Generator** will be performed, in an attempt to identify the causes\n",
        "of its possible underperforming."
      ],
      "metadata": {
        "id": "bcUEHJ9ccCP8"
      },
      "id": "bcUEHJ9ccCP8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Undercomplete AutoEncoder\n",
        "\n",
        "In order to determine if the architecture could be capable of reconstructing\n",
        "the images of the training set, we'll train an **Undercomplete AutoEncoder**\n",
        "with a similar architecture.\n",
        "\n",
        "In particular, if the **AutoEncoder** is capable of reconstructing the images\n",
        "of the training set, then the **CVAE** should also be able to do so, meaning\n",
        "that the design of the **CVAE** is correct, but there is a problem in its implementation. Otherwise, there is a problem in the design of the **CVAE** (e.g. the model may be too small for the task...).\n",
        "\n",
        "Basically, we are debugging the **CVAE** by training parts of its architecture as standalone components, in order to understand which of them are not behaving as expected, if any."
      ],
      "metadata": {
        "id": "ze_JJjwhdA5I"
      },
      "id": "ze_JJjwhdA5I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Architecture\n",
        "\n",
        "The architecture of the **AutoEncoder** is a simplified version of the\n",
        "architecture of the **CVAE**.\n",
        "\n",
        "In particular, an **AutoEncoder** does not require additional knowledge about the processed input. In other words, it is no longer required to pass the landscape type $y$ as input to the model.\n",
        "Moreover, the inputs are mapped into the latent space directly, without\n",
        "intermediate probability distributions.\n",
        "\n",
        "As a consequence, the **Encoder Core** and the **Decoder Core**, that were already introduced in the architecture of the **CVAE**, can be used\n",
        "directly as the **Encoder** and **Decoder** of the **AutoEncoder**.\n",
        "\n",
        "Such architecture is shown in the figure below."
      ],
      "metadata": {
        "id": "sk_vgicLebc4"
      },
      "id": "sk_vgicLebc4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16u5at1lFxPsnyQ5M2ohZgbI33JwKIETu\" width=\"60%\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "Z1woad6nhbml"
      },
      "id": "Z1woad6nhbml"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, the **Encoder Core** process a landscape image $x$, producing\n",
        "its corresponding code $z$.\n",
        "\n",
        "Then, the code $z$ is fed to the **Decoder Core**, which tries to produce a reconstruction $x'$ of the original image $x$ from the limited information contained in $z$."
      ],
      "metadata": {
        "id": "Wdw5EUSJjwUQ"
      },
      "id": "Wdw5EUSJjwUQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Definition\n",
        "\n",
        "In order to build the **AutoEncoder**, let's start by defining the **Encoder** as the **Encoder Core**."
      ],
      "metadata": {
        "id": "mNTtOH4l7qp8"
      },
      "id": "mNTtOH4l7qp8"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ae_encoder(image_shape, activation):\n",
        "    input = layers.Input(name=\"OriginalImage\", shape=image_shape)\n",
        "    output = build_encoder_core(activation)(input)\n",
        "    return keras.Model(name = \"AE_Encoder\", inputs=input, outputs=output)"
      ],
      "metadata": {
        "id": "eZ50bEq6h9e2"
      },
      "id": "eZ50bEq6h9e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_encoder = build_ae_encoder(\n",
        "    image_shape=image_size,\n",
        "    activation=cnn_activation\n",
        ")"
      ],
      "metadata": {
        "id": "G3Q77ld2cGMm"
      },
      "id": "G3Q77ld2cGMm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's define the **Decoder** as the **Decoder Core**."
      ],
      "metadata": {
        "id": "K9WtO09n7MB4"
      },
      "id": "K9WtO09n7MB4"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ae_decoder(feature_shape, activation, output_activation):\n",
        "    return build_decoder_core(feature_shape, activation, output_activation)"
      ],
      "metadata": {
        "id": "_MWez0mvjWws"
      },
      "id": "_MWez0mvjWws",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_decoder = build_ae_decoder(\n",
        "    feature_shape=ae_encoder.get_layer(\"Encoder_Core\").get_layer(\"OriginalImage_Features\").input.shape[1:],\n",
        "    activation=cnn_activation,\n",
        "    output_activation=output_activation\n",
        ")"
      ],
      "metadata": {
        "id": "VsHgp6NmP-uE"
      },
      "id": "VsHgp6NmP-uE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's define the **AutoEncoder** itself, which contains both the **Encoder** and the **Decoder**."
      ],
      "metadata": {
        "id": "OF3T_HIV7YAH"
      },
      "id": "OF3T_HIV7YAH"
    },
    {
      "cell_type": "code",
      "source": [
        "ae = keras.Model(name='AE', inputs=ae_encoder.input, outputs=ae_decoder(ae_encoder.output))"
      ],
      "metadata": {
        "id": "PAFjc8sLP-fP"
      },
      "id": "PAFjc8sLP-fP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at a description of the model."
      ],
      "metadata": {
        "id": "rcQdqrY0oPby"
      },
      "id": "rcQdqrY0oPby"
    },
    {
      "cell_type": "code",
      "source": [
        "ae.summary()"
      ],
      "metadata": {
        "id": "PjLijKRwmouC"
      },
      "id": "PjLijKRwmouC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, let's look at its **computation graph**."
      ],
      "metadata": {
        "id": "MT-VOUPEoi32"
      },
      "id": "MT-VOUPEoi32"
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(ae, show_shapes=True, show_layer_names=True, expand_nested=True, to_file=directories[\"images\"]+\"ae.png\")"
      ],
      "metadata": {
        "id": "rTK9WR5ooHo2"
      },
      "id": "rTK9WR5ooHo2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input Preparation\n",
        "\n",
        "Before training the model, it is necessary to prepare the dataset appropriately\n",
        "for the training session.\n",
        "\n",
        "For training the **AutoEncoder**, we'll be using a **built-in** loss, so the training set should contain batches of tuples of (**Input**, **TargetOutput**).\n",
        "Since the **TargetOutput** is the **Input** itself for an **AutoEncoder**, the\n",
        "training set will actually contain batches of tuples of (**Input**, **Input**)."
      ],
      "metadata": {
        "id": "JveTXfQLpEAB"
      },
      "id": "JveTXfQLpEAB"
    },
    {
      "cell_type": "code",
      "source": [
        "def to_ae_input(xi, yi): return (xi, xi)"
      ],
      "metadata": {
        "id": "97wFVEyTpHlY"
      },
      "id": "97wFVEyTpHlY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the training, validation and test sets can be prepared similarly with respect to those\n",
        "used for the **CVAE**."
      ],
      "metadata": {
        "id": "eLzLGFkPro8N"
      },
      "id": "eLzLGFkPro8N"
    },
    {
      "cell_type": "code",
      "source": [
        "ae_training_set = prepare_training_set(training_set, prepare_fn=to_ae_input, cache_name=\"ae_training_set\")\n",
        "ae_validation_set = prepare_validation_set(validation_set, prepare_fn=to_ae_input, cache_name=\"ae_validation_set\")\n",
        "ae_test_set = prepare_test_set(test_set, prepare_fn=to_ae_input, cache_name=\"ae_test_set\")"
      ],
      "metadata": {
        "id": "fWnEvkv5rzpa"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fWnEvkv5rzpa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loss Function\n",
        "\n",
        "The loss of an **AutoEncoder** is simply equal to the **Reconstruction Error**.\n",
        "However, since the **Reconstruction Error** is the only component of the loss,\n",
        "it does not require to be weighted by the number of samples in the training set.\n",
        "Therefore, we can use the built-in **MSE** cost function directly."
      ],
      "metadata": {
        "id": "GcvFSYvPsO9R"
      },
      "id": "GcvFSYvPsO9R"
    },
    {
      "cell_type": "code",
      "source": [
        "ae_loss = \"mse\""
      ],
      "metadata": {
        "id": "rDHKGr9OsQGR"
      },
      "id": "rDHKGr9OsQGR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image Reconstruction Callback\n",
        "\n",
        "In order to monitor the reconstruction of the images produced by the **AutoEncoder** during the training, here a callback is provided to\n",
        "reconstruct a batch of images at the end of each epoch."
      ],
      "metadata": {
        "id": "MDp9z185triH"
      },
      "id": "MDp9z185triH"
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageReconstructionCallback(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    A callback that reconstructs the specified 'dataset' of images\n",
        "    at the end of each epoch, displaying them in a grid of 'n_rows'\n",
        "    and 'n_cols'.\n",
        "    \"\"\"\n",
        "    def __init__(self, ae, dataset, n_rows, n_cols, generateWhen = lambda epoch: True):\n",
        "        keras.callbacks.Callback.__init__(self)\n",
        "        self.ae = ae\n",
        "        self.dataset = dataset\n",
        "        self.n_rows = n_rows\n",
        "        self.n_cols = n_cols\n",
        "        self.generateWhen = generateWhen\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        actual_epoch = epoch + 1\n",
        "        if self.generateWhen(actual_epoch):\n",
        "            print()\n",
        "            log(\"Reconstructing images for epoch\", actual_epoch, \"...\")\n",
        "            reconstructed_dataset = self.dataset.map(lambda x, y: (self.ae(tf.expand_dims(x, axis=0), training=False)[0], y))\n",
        "            original_vs_reconstructed_dataset = tf.data.Dataset.choose_from_datasets([self.dataset, reconstructed_dataset], tf.data.Dataset.range(2).repeat())\n",
        "            show_dataset_sample(\n",
        "                dataset=original_vs_reconstructed_dataset,\n",
        "                n_rows=self.n_rows,\n",
        "                n_cols=self.n_cols,\n",
        "                show_label_probability=False,\n",
        "            )"
      ],
      "metadata": {
        "id": "fNW0Y_0Vtq1p"
      },
      "id": "fNW0Y_0Vtq1p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Compilation\n",
        "\n",
        "The last step before training is to compile the model, choosing the type of\n",
        "training algorithm and loss that will be used for training the model."
      ],
      "metadata": {
        "id": "_8mNlwaEx-Gu"
      },
      "id": "_8mNlwaEx-Gu"
    },
    {
      "cell_type": "code",
      "source": [
        "ae.compile(loss=ae_loss, optimizer=optimizer())"
      ],
      "metadata": {
        "id": "mjHYRhIKs5iE"
      },
      "id": "mjHYRhIKs5iE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training\n",
        "\n",
        "Finally, let's train the model using the prepared training and validation sets."
      ],
      "metadata": {
        "id": "Q0ONfmdaySHf"
      },
      "id": "Q0ONfmdaySHf"
    },
    {
      "cell_type": "code",
      "source": [
        "ae_history = ae.fit(\n",
        "    ae_training_set,\n",
        "    validation_data=ae_validation_set,\n",
        "    epochs=epoch_count,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor='loss', patience=patience, restore_best_weights=True),\n",
        "        keras.callbacks.ModelCheckpoint(monitor='loss', filepath=directories[\"model\"]+'ae.checkpoint.keras'),\n",
        "        ImageReconstructionCallback(ae, training_set.take(16), 4, 4, generateWhen=lambda epoch: epoch%10==1 or epoch==epoch_count),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "pDHcLhZbyTxV"
      },
      "id": "pDHcLhZbyTxV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the loss on the training and validation sets."
      ],
      "metadata": {
        "id": "2JQPUT4CyuSp"
      },
      "id": "2JQPUT4CyuSp"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(ae_history, metrics=['val_loss'])"
      ],
      "metadata": {
        "id": "UMHKpML-yve9"
      },
      "id": "UMHKpML-yve9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the last step of training, let's save the model, so that its weights may be\n",
        "loaded in the future, for using the model or continuing its training."
      ],
      "metadata": {
        "id": "nNRQWeZ-y15A"
      },
      "id": "nNRQWeZ-y15A"
    },
    {
      "cell_type": "code",
      "source": [
        "ae.save(directories[\"model\"]+'ae.keras')"
      ],
      "metadata": {
        "id": "3qgTS83ey2rQ"
      },
      "id": "3qgTS83ey2rQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations\n",
        "\n",
        "`[11-22-2023]`\n",
        "\n",
        "In this revision of the notebook, we can see that the **Encoder Core** and the **Decoder Core** are complex enough to reconstruct the original images flawlessly in the latest epochs.\n",
        "\n",
        "Knowing that, we can infer that the problem with the **CVAE** should be either\n",
        "in its inputs or in the latent space (e.g. the dimension of the latent space...)."
      ],
      "metadata": {
        "id": "rC2pXHErihbw"
      },
      "id": "rC2pXHErihbw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variational AutoEncoder\n",
        "\n",
        "The previous analysis can be deepened further by training a **Variational\n",
        "AutoEncoder** similar to the **CVAE**."
      ],
      "metadata": {
        "id": "yos2OCBWkeQl"
      },
      "id": "yos2OCBWkeQl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Architecture\n",
        "\n",
        "The architecture of the **Variational AutoEncoder** is the same as the\n",
        "architecture of the **CVAE**, except that it is not required to pass the landscape type $y$ as input to the model.\n",
        "\n",
        "Such architecture is shown in the figure below."
      ],
      "metadata": {
        "id": "PgSQcHCH-vlQ"
      },
      "id": "PgSQcHCH-vlQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ObHbDOn_6lpHgECZflQsNuQTpc9cCAeh\" width=\"80%\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "nFW0mimDkeQr"
      },
      "id": "nFW0mimDkeQr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Definition\n",
        "\n",
        "In order to build the **Variational AutoEncoder**, let's start by defining the **Encoder** as the **Encoder Core**.\n",
        "\n",
        "Contrary to an **AutoEncoder**, the **Encoder** of a **Variational AutoEncoder**\n",
        "produces the means and the variances of multinormal distributions projected in the latest space, just like the **CVAE**."
      ],
      "metadata": {
        "id": "0yK7UEh8keQr"
      },
      "id": "0yK7UEh8keQr"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vae_encoder(image_shape, condition_size, code_size, activation, latent_activation):\n",
        "    input = layers.Input(name=\"OriginalImage\", shape=image_shape)\n",
        "    x = build_encoder_core(activation)(input)\n",
        "\n",
        "    mean = layers.Dense(name='Mean', units=code_size, activation=latent_activation)(x)\n",
        "    log_variance = layers.Dense(name='LogVariance', units=code_size, activation=latent_activation)(x)\n",
        "\n",
        "    return keras.Model(name=\"VAE_Encoder\", inputs=input, outputs=[mean, log_variance])"
      ],
      "metadata": {
        "id": "g0Tg1mgckeQr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "g0Tg1mgckeQr"
    },
    {
      "cell_type": "code",
      "source": [
        "vae_encoder = build_vae_encoder(\n",
        "    image_shape=image_size,\n",
        "    condition_size=condition_size,\n",
        "    code_size=code_size,\n",
        "    activation=cnn_activation,\n",
        "    latent_activation=latent_activation\n",
        ")"
      ],
      "metadata": {
        "id": "7ltPxxbRkeQs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7ltPxxbRkeQs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's define the **Decoder** as the **Decoder Core**."
      ],
      "metadata": {
        "id": "ZeqlPoDRBAuU"
      },
      "id": "ZeqlPoDRBAuU"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vae_decoder(feature_shape, activation, output_activation):\n",
        "    input = layers.Input(name=\"Code\", shape=code_size)\n",
        "    x = layers.Dense(name='GeneratedImage_Features', units=feature_shape.num_elements())(input)\n",
        "    output = build_decoder_core(feature_shape, activation, output_activation)(x)\n",
        "    return keras.Model(name='VAE_Decoder', inputs=input, outputs=output)"
      ],
      "metadata": {
        "id": "nBpRqCm4keQr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nBpRqCm4keQr"
    },
    {
      "cell_type": "code",
      "source": [
        "vae_decoder = build_vae_decoder(\n",
        "    feature_shape=vae_encoder.get_layer(\"Encoder_Core\").get_layer(\"OriginalImage_Features\").input.shape[1:],\n",
        "    activation=cnn_activation,\n",
        "    output_activation=output_activation\n",
        ")"
      ],
      "metadata": {
        "id": "OaSZJ-fTkeQs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OaSZJ-fTkeQs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's define the **Variational AutoEncoder** itself, which contains both the **Encoder** and the **Decoder**."
      ],
      "metadata": {
        "id": "3zH3XGzRBW62"
      },
      "id": "3zH3XGzRBW62"
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE:\n",
        "    def __init__(self, encoder, decoder, name=\"VAE\"):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        self.input = self.encoder.input\n",
        "        self.mean = self.encoder.output[0]\n",
        "        self.log_variance = self.encoder.output[1]\n",
        "        self.code_size = self.mean.shape[1]\n",
        "        self.sampling = MultinormalSamplingLayer(name='Sampling')([self.mean, self.log_variance])\n",
        "        self.output = self.decoder(self.sampling)\n",
        "\n",
        "        self.model = keras.Model(name=name, inputs=self.input, outputs=self.output)\n",
        "\n",
        "    def generate_single_data(self, code):\n",
        "        \"\"\"Generate a new image corresponding to the specified 'code'.\"\"\"\n",
        "        code_batch = tf.reshape(code, shape=(1, self.code_size))\n",
        "        return self.generate_batch_data(code_batch)[0]\n",
        "\n",
        "    def generate_batch_data(self, code_batch):\n",
        "        \"\"\"Generate some new images corresponding to the specified batch of codes.\"\"\"\n",
        "        return self.decoder([code_batch], training=False)"
      ],
      "metadata": {
        "id": "iyhb4L7U_szq"
      },
      "id": "iyhb4L7U_szq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VAE(vae_encoder, vae_decoder)"
      ],
      "metadata": {
        "id": "hBnA3z-ekeQs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hBnA3z-ekeQs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at a description of the model."
      ],
      "metadata": {
        "id": "5w59LvGykeQs"
      },
      "id": "5w59LvGykeQs"
    },
    {
      "cell_type": "code",
      "source": [
        "vae.model.summary()"
      ],
      "metadata": {
        "id": "TmyYuxCIkeQs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TmyYuxCIkeQs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, let's look at its **computation graph**."
      ],
      "metadata": {
        "id": "tonbu2Y1keQs"
      },
      "id": "tonbu2Y1keQs"
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(vae.model, show_shapes=True, show_layer_names=True, expand_nested=True, to_file=directories[\"images\"]+\"vae.png\")"
      ],
      "metadata": {
        "id": "QFLtDZ0tkeQs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QFLtDZ0tkeQs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input Preparation\n",
        "\n",
        "Before training the model, it is necessary to prepare the dataset appropriately\n",
        "for the training session.\n",
        "\n",
        "For training the **Variational AutoEncoder**, we'll be using the same loss as the **CVAE**, so the training set should contain batches of singleton tuples of (**Input**, ). However, contrary to the **CVAE**, the **Input** does not include\n",
        "the landscape type $y$."
      ],
      "metadata": {
        "id": "rKHfMq0bkeQs"
      },
      "id": "rKHfMq0bkeQs"
    },
    {
      "cell_type": "code",
      "source": [
        "def to_vae_input(xi, yi): return (xi,)"
      ],
      "metadata": {
        "id": "h5GkyqdLkeQs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "h5GkyqdLkeQs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the training, validation and test sets can be prepared similarly to those\n",
        "used for training the **CVAE**."
      ],
      "metadata": {
        "id": "VG3hJD2ekeQs"
      },
      "id": "VG3hJD2ekeQs"
    },
    {
      "cell_type": "code",
      "source": [
        "vae_training_set = prepare_training_set(training_set, prepare_fn=to_vae_input, cache_name=\"vae_training_set\")\n",
        "vae_validation_set = prepare_validation_set(validation_set, prepare_fn=to_vae_input, cache_name=\"vae_validation_set\")\n",
        "vae_test_set = prepare_test_set(test_set, prepare_fn=to_vae_input, cache_name=\"vae_test_set\")"
      ],
      "metadata": {
        "id": "_6DDvfEfkeQs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_6DDvfEfkeQs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loss Function\n",
        "\n",
        "The loss of a **Variational AutoEncoder** is the same loss of the **CVAE**,\n",
        "that is the sum of the **Reconstruction Error** and the **Regularization Error**.\n",
        "\n",
        "Let's start by defining these separate losses."
      ],
      "metadata": {
        "id": "lzbkuF-zkeQs"
      },
      "id": "lzbkuF-zkeQs"
    },
    {
      "cell_type": "code",
      "source": [
        "vae_regul_error = regularization_error(vae.mean, vae.log_variance, kl_coefficient)\n",
        "vae_recon_error = reconstruction_error(vae.model.input, vae.model.output, training_set_size)"
      ],
      "metadata": {
        "id": "DiwQh4MAA-9g"
      },
      "id": "DiwQh4MAA-9g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's configure the model to train using the same loss and metrics of the\n",
        "**CVAE**."
      ],
      "metadata": {
        "id": "uOQJkzc1CJds"
      },
      "id": "uOQJkzc1CJds"
    },
    {
      "cell_type": "code",
      "source": [
        "vae.model.add_loss(cvae_loss(vae_recon_error, vae_regul_error, recon_coefficient))\n",
        "vae.model.add_metric(vae_recon_error, name=\"reconstruction_error\")\n",
        "vae.model.add_metric(vae_regul_error, name=\"regularization_error\")"
      ],
      "metadata": {
        "id": "YPF-kuxvkeQs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YPF-kuxvkeQs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Compilation\n",
        "\n",
        "The last step before training is to compile the model, choosing the type of\n",
        "training algorithm that will be used for training the model."
      ],
      "metadata": {
        "id": "E4CX6LvokeQt"
      },
      "id": "E4CX6LvokeQt"
    },
    {
      "cell_type": "code",
      "source": [
        "vae.model.compile(optimizer=optimizer())"
      ],
      "metadata": {
        "id": "yvxOEvKBkeQt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yvxOEvKBkeQt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training\n",
        "\n",
        "Finally, let's train the model using the prepared training and validation sets."
      ],
      "metadata": {
        "id": "egTB27fYkeQt"
      },
      "id": "egTB27fYkeQt"
    },
    {
      "cell_type": "code",
      "source": [
        "vae_history = vae.model.fit(\n",
        "    vae_training_set,\n",
        "    validation_data=vae_validation_set,\n",
        "    epochs=epoch_count,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor='loss', patience=patience, restore_best_weights=True),\n",
        "        keras.callbacks.ModelCheckpoint(monitor='loss', filepath=directories[\"model\"]+'vae.checkpoint.keras'),\n",
        "        ImageReconstructionCallback(vae.model, training_set.take(16), 4, 4, generateWhen=lambda epoch: epoch%10==1 or epoch==epoch_count),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "JEBy4_r8keQt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JEBy4_r8keQt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the loss on the training and validation sets."
      ],
      "metadata": {
        "id": "ZeFK5_HukeQt"
      },
      "id": "ZeFK5_HukeQt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffPd9SaENVwJ"
      },
      "outputs": [],
      "source": [
        "plot_history(history=vae_history, metrics=history_metrics, skip=1)"
      ],
      "id": "ffPd9SaENVwJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the last step of training, let's save the model, so that its weights may be\n",
        "loaded in the future, for using the model or continuing its training."
      ],
      "metadata": {
        "id": "hL8B-wLTkeQt"
      },
      "id": "hL8B-wLTkeQt"
    },
    {
      "cell_type": "code",
      "source": [
        "vae.model.save(directories[\"model\"]+'vae.keras')"
      ],
      "metadata": {
        "id": "CfKue6NckeQt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CfKue6NckeQt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generation\n",
        "\n",
        "Contrary to the **AutoEncoder**, the **Variational AutoEncoder** can also be used for generating new landscape images, since it is trained to discover a **regular** latent space during the compression of the original images.\n",
        "\n",
        "In order to do so, it is possible to apply its **Decoder** on random samples\n",
        "extracted from a standard multinormal distribution (which is the same constraint enforced on the latent space during training)."
      ],
      "metadata": {
        "id": "R-cD-Zugcegs"
      },
      "id": "R-cD-Zugcegs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's use the **Variational AutoEncoder** to generate some images of landscapes.\n",
        "\n",
        "> _**Note**: in a **Variational AutoEncoder**, all the landscape types are mixed within the same latent space, so when an image is generated from a random point in the latent space, it is not possible to know its landscape\n",
        "type beforehand. In a **CVAE** instead, a latent space is produced for each landscape type, letting the user decide the landscape type of the generated images._"
      ],
      "metadata": {
        "id": "zfbb-W9wU8FM"
      },
      "id": "zfbb-W9wU8FM"
    },
    {
      "cell_type": "code",
      "source": [
        "latent_points_count = 25\n",
        "latent_points = tf.random.normal(shape=(latent_points_count, vae.code_size))\n",
        "generated_images = tf.data.Dataset.from_tensor_slices((vae.generate_batch_data(latent_points), None))"
      ],
      "metadata": {
        "id": "_Tblg3oudsyU"
      },
      "id": "_Tblg3oudsyU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's take a look at the landscapes generated by the **Variational AutoEncoder**."
      ],
      "metadata": {
        "id": "iA7kLywQYEAt"
      },
      "id": "iA7kLywQYEAt"
    },
    {
      "cell_type": "code",
      "source": [
        "show_dataset_sample(generated_images, n_rows=5, n_cols=5, image_header=False)"
      ],
      "metadata": {
        "id": "P3fSLfxb4v_g"
      },
      "id": "P3fSLfxb4v_g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations\n",
        "\n",
        "`[11-22-2023]`\n",
        "\n",
        "In this revision of the notebook, we can see that the **Variational AutoEncoder** is not complex enough to reconstruct the original images as well\n",
        "as the simpler **AutoEncoder**. That is probably due to a combination of the\n",
        "dimension of the latent space and the constraints on its regularity, making the problem of reconstructing the original images more difficult to solve.\n",
        "\n",
        "Knowing that, we can infer that the **CVAE** may suffer from the same problems.\n",
        "\n",
        "Moreover, the generation capability of the **Variational AutoEncoder** trained on a subset of the dataset is only slightly better with the respect to the **CVAE** trained on the whole dataset."
      ],
      "metadata": {
        "id": "KJDbPRPxkeQt"
      },
      "id": "KJDbPRPxkeQt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Results\n",
        "\n",
        "Let's take a look at the images generated by different configurations of the model trained in this notebook.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TMftXG3Db01v"
      },
      "id": "TMftXG3Db01v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E100-D2-Tanh-KL1**\n",
        "These are the images generated after 100 epochs when setting the code size to 2 (i.e. bidimensional latent space), using **tanh** as the latent activation\n",
        "function and a **KL Coefficient** equals to 1.\n",
        "\n",
        "**Coasts**\n",
        "\n",
        "![Generated Coasts](https://drive.google.com/uc?export=view&id=1d1Epqh_SHclJ54jFfBrAipm0UeDpH24r)\n",
        "\n",
        "**Deserts**\n",
        "\n",
        "![Generated Deserts](https://drive.google.com/uc?export=view&id=1NgsqCpnekM-lzPkjYn3R_BLKwk1O5z-f)\n",
        "\n",
        "**Forests**\n",
        "\n",
        "![Generated Forests](https://drive.google.com/uc?export=view&id=1ALwJzCkUlWSe4utgPuHN5UIX_r1GhnOo)\n",
        "\n",
        "**Glaciers**\n",
        "\n",
        "![Generated Glaciers](https://drive.google.com/uc?export=view&id=1YxafLzIoH1mM_fjW4u0jcETLbO0pvCD2)\n",
        "\n",
        "**Mountains**\n",
        "\n",
        "![Generated Mountains](https://drive.google.com/uc?export=view&id=13pKtBCvxeYFRFpSYS86m802t3LZ1Ottc)"
      ],
      "metadata": {
        "id": "59XR7eRLgO6g"
      },
      "id": "59XR7eRLgO6g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E100-D10-Tanh-KL1**\n",
        "These are the images generated after 100 epochs when setting the code size to 10, using **tanh** as the latent activation function and a **KL Coefficient** equals to 1.\n",
        "\n",
        "**Coasts**\n",
        "\n",
        "![Generated Coasts](https://drive.google.com/uc?export=view&id=1In-y4AsHOpFigWCZBISJyyDq_c4EXHu3)\n",
        "\n",
        "**Deserts**\n",
        "\n",
        "![Generated Deserts](https://drive.google.com/uc?export=view&id=1t_m4d-vJGkyHrsTGsfViuVPkA7hQdFSI)\n",
        "\n",
        "**Forests**\n",
        "\n",
        "![Generated Forests](https://drive.google.com/uc?export=view&id=1tgcLDxWCFKFKryEjXx9CY_UcM1oRMVMX)\n",
        "\n",
        "**Glaciers**\n",
        "\n",
        "![Generated Glaciers](https://drive.google.com/uc?export=view&id=10u_zqAkKoSkm7kL9zgXg9WuaT6yTobKi)\n",
        "\n",
        "**Mountains**\n",
        "\n",
        "![Generated Mountains](https://drive.google.com/uc?export=view&id=1z_h21xqEvl8BAJEXxbZJFbLkGwFrAOSF)"
      ],
      "metadata": {
        "id": "S5S97r4agbPf"
      },
      "id": "S5S97r4agbPf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E100-D100-Tanh-KL1**\n",
        "These are the images generated after 100 epochs when setting the code size to 100, using **tanh** as the latent activation function and a **KL Coefficient** equals to 1.\n",
        "\n",
        "**Coasts**\n",
        "\n",
        "![Generated Coasts](https://drive.google.com/uc?export=view&id=1myald24ny3kpAVv4S9zWLHZ58WYXyD3b)\n",
        "\n",
        "**Deserts**\n",
        "\n",
        "![Generated Deserts](https://drive.google.com/uc?export=view&id=10E3-dnCenN_dp5vz6j-kWt7fdzrGWBVe)\n",
        "\n",
        "**Forests**\n",
        "\n",
        "![Generated Forests](https://drive.google.com/uc?export=view&id=1YlCwGYcOp7-y8JI_SzAIO7VPMcvj-8E1)\n",
        "\n",
        "**Glaciers**\n",
        "\n",
        "![Generated Glaciers](https://drive.google.com/uc?export=view&id=1J4kJmywIosERlyQUiB_agjgPlQ4MPOh-)\n",
        "\n",
        "**Mountains**\n",
        "\n",
        "![Generated Mountains](https://drive.google.com/uc?export=view&id=1xOzrPmcNKr1-VL8HzZBV-ZMTqmCjsHI0)"
      ],
      "metadata": {
        "id": "bd__5db4gobV"
      },
      "id": "bd__5db4gobV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E100-D200-None-KL1**\n",
        "These are the images generated after 100 epochs when setting the code size to 200, using no latent activation function and a **KL Coefficient** equals to 1.\n",
        "This was the best considering overall performances.\n",
        "\n",
        "> _**Note**: the latent activation function was set to `None` because it was\n",
        "constraining the latent space, positioning the mean of the multinormal distributions within a D-dimensional hypercube with length equals to 2, therefore implicitly implying some regularization in the latent space._\n",
        "\n",
        "**Coasts**\n",
        "\n",
        "![Generated Coasts](https://drive.google.com/uc?export=view&id=1gvoBPuM0aIwNjWMHVaxg8qb2XtsOc1Y7)\n",
        "\n",
        "**Deserts**\n",
        "\n",
        "![Generated Deserts](https://drive.google.com/uc?export=view&id=1t_Ua9vypo699d7k6xdConfU5HJJ3TqEV)\n",
        "\n",
        "**Forests**\n",
        "\n",
        "![Generated Forests](https://drive.google.com/uc?export=view&id=1luniZQE4QTyBCSxmIzBr0eGIKgBOJ1yh)\n",
        "\n",
        "**Glaciers**\n",
        "\n",
        "![Generated Glaciers](https://drive.google.com/uc?export=view&id=1FiBCRz2YBbVezsgXro-bYb_gy-hvUEko)\n",
        "\n",
        "**Mountains**\n",
        "\n",
        "![Generated Mountains](https://drive.google.com/uc?export=view&id=1n83MOIz4bBvrtPUR1JWKvgus6udkDIeb)"
      ],
      "metadata": {
        "id": "q9T1lVAc357r"
      },
      "id": "q9T1lVAc357r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E100-D200-None-KL0.1**\n",
        "\n",
        "These are the images generated after 100 epochs when setting the code size to 200, using no latent activation function and a **KL Coefficient** equals to 0.1.\n",
        "This was the best considering reconstruction performances but also the worst\n",
        "considering regularization performances.\n",
        "\n",
        "**Coasts**\n",
        "\n",
        "![Generated Coasts](https://drive.google.com/uc?export=view&id=1kqb6vCX4iXiWe8j7JHEMwKVFivb9psjQ)\n",
        "\n",
        "**Deserts**\n",
        "\n",
        "![Generated Deserts](https://drive.google.com/uc?export=view&id=1vSFEV32sxkE8taoV1yhShIfuskgZNtsm)\n",
        "\n",
        "**Forests**\n",
        "\n",
        "![Generated Forests](https://drive.google.com/uc?export=view&id=1tNchtk6cQJeJyNu1GGzHEFFgkmbSH1th)\n",
        "\n",
        "**Glaciers**\n",
        "\n",
        "![Generated Glaciers](https://drive.google.com/uc?export=view&id=1tNchtk6cQJeJyNu1GGzHEFFgkmbSH1th)\n",
        "\n",
        "**Mountains**\n",
        "\n",
        "![Generated Mountains](https://drive.google.com/uc?export=view&id=1b4TGREO9CF4LV_gfzlunAvKMU4css40e)"
      ],
      "metadata": {
        "id": "P4XKUabT7DQH"
      },
      "id": "P4XKUabT7DQH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "`[11-22-2023]`\n",
        "\n",
        "From the images above, we can observe that increasing the size of the latent space allows the model to provide finer details in the generated landscapes,\n",
        "achieving somewhat **impressionistic representations of landscapes** when the latent space has a hundred dimensions.\n",
        "\n",
        "Of course this comes at a cost. In fact, increasing the dimensions of the latent space also causes an increase of the model complexity, reaching levels of\n",
        "complexity that make the model very difficult to train or even create in standard environments.\n",
        "\n",
        "Another consideration is the possibility of balancing the reconstruction\n",
        "capability of the model with the regularity of its latent space.\n",
        "\n",
        "For example, it is possible to reduce the weight of the **Regularization Error** (namely the **KL Coefficient**), giving to the model more flexibility on the generation of the latent space when reconstructing the images.\n",
        "However, while this would increase the reconstruction capability of the model, it would also decrease the regularity of the latent space as a tradeoff, increasing the chances of generating images unrelated to landscapes (e.g. blank images...).\n",
        "In other words, this would increase the amount of details in the images that\n",
        "are correctly generated, but it would also increase the chances of incorrectly\n",
        "generated images."
      ],
      "metadata": {
        "id": "vFw8MqeLgp7G"
      },
      "id": "vFw8MqeLgp7G"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}